%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

% Import additional packages
\usepackage{graphicx}
\usepackage{enumitem}
% \usepackage{todonotes}
\usepackage{tabularx}
% \usepackage{fixltx2e}
\usepackage{spverbatim}

\usepackage{amsmath}
\usepackage{algorithm,algorithmic}


\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{BERT-based Rumour Identification and Analysis for Twitter Posts}

\author{ Student ID: 1133751 \\
 COMP90042 Natural Language Processing \\
 The University of Melbourne
}

%\author{Matthias Bachfischer \\
% Student ID: 1133751 \\
% COMP90042 Natural Language Processing \\
 % \texttt{mbachfischer@unimelb.edu.au} 
 % }

\date{}

\begin{document}
\maketitle

\section{Introduction}
With an increase in the adoption of social media as a news source, it has become very easy for villains to share false information with a large audience. This can lead to the spread of so-called "fake news" and rumours that may manipulate the public's opinion. To combat this issue, there is a need for automated solutions to perform rumour identification.
\newline
Identifying rumours is a challenging problem because of their dynamically evolving nature and the ambiguity concerning what should be considered a rumour and what not \citep{RN675}. This makes the task of automatically identifying rumours challenging and requires the use of elaborate methods to differentiate rumours from non-rumours. 
% A post shared on Twitter may contain an incorrect claim, but may not have been shared with the intent to spread misinformation, while another post may contain misleading conspiracy theories as well some unrelated, but correct facts. 


\section{Related Work}
In recent years, the topic of rumour identification and analysis has attracted significant attention and has been subject to studies in shared tasks such as RumourEval 2017 \citep{RN690} and RumourEval 2019 \citep{RN691}.  Previous work on the topic of rumour identification for tweets can be divided into four major categories: \textit{text-based},  \textit{feature-based}, \textit{propagation-based} and \textit{stance-based} methods.
\newline
\textit{Text-based methods} leverage the textual contents of the tweets. Tweets often have unique characteristics with regard to their language and syntax which can be used for rumour detection. An example of this method is \citet{RN671}, who used a GRU-based architecture to perform rumour detection at an very early stage of the propagation.
\newline
\textit{Feature-based} methods extend this approach to also include non-textual features such as user profile data for rumour detection. In \citet{RN481}, word sentiments and phrases with support \& denial terms were used for rumour detection, while in \citet{RN673}, a variety of both text and user-based features from usersâ€™ text response and profiles were extracted before training a CNN classifier for rumour identification. 
\newline
\textit{Propagation-based methods} aim at incorporating the information propagation path for rumour classification. A recent approach was proposed by \citet{RN679} where an extended tree-structured recursive neural network (RvNN) was used to model information propagation. 
In \citet{RN672}, a novel architecture that uses Recurrent and Convolutional Neural Networks to classify propagation paths of events on Twitter was used. \citet{RN669} were able to incorporate user interactions into the propagation classification via a tree transformer model.
\newline
\textit{Stance-based methods} try to determine the rumour stance of a post with respect to the previous tweet and the source tweet. A system that leverages BERT for stance identification was published by \citet{RN665}. Another architecture that uses a combination of a CNN and BERT architecture was presented by \citet{RN480}.

\section{Dataset}
The task of the COMP90042 2021 project was to develop a rumour detection system and analyze the nature of rumours that are being propagated on Twitter.
The dataset for this task was published by the COMP90042 teaching team and consists of a set of source tweets and their replies (incl. corresponding metadata) that has been extracted from the Twitter API.
The training set contains a total of 4641 events which have been labeled as either \textit{RUMOUR} or \textit{NON-RUMOUR}. To evaluate the performance of the system, an additional development set has been made available.
\newline
The training set is imbalanced and the majority of the tweets (approx. 66\%) belong to the non-rumour class, whereas the remaining data (approx. 34\%) belongs to the rumour class.
% A detailed breakdown of the class distribution is presented in Table \ref{tab:dataset_class_distribution}.

%\begin{table}
%\centering
%\setlength\tabcolsep{2pt}
%\begin{tabular}{lccc}
%\hline
%\textbf{Dataset}         & Rumour & Non-Rumour & Total \\ %\hline
%Training set    & 1583   & 3058       & 4641  \\
%Development set & 187    & 393        & 580   \\
%Test set        & -      & -          & 581   \\ \hline
%\end{tabular}
%\caption{Class distribution for rumour detection dataset}
%\label{tab:dataset_class_distribution}
%\end{table}


\section{Experimental Setup}
All systems used in this research were implemented in Python and make use of the Transformers library \citep{RN682} which provides access to various Transformer-based architectures such as BERT, RoBERTa, and DistilBERT. 
\newline
The experiments reported in this paper were performed on a VM instance within the Google Cloud Platform running Debian 10 with a NVIDIA Tesla K80 GPU.


\section{Task 1 - Rumour Detection}
\label{sec:task_1_rumour_detection}
The objective of task 1 was to build a binary classifier that can reliably predict whether a given tweet represents a rumour or not.
\newline
For this task, we have implemented three classification systems: A BERT-based implementation that uses the textual representation of tweets (we refer to this architecture as \textit{PureBERT}) and an extension of this architecture that combines the textual features with tabular data (we refer to this architecture as \textit{MultimodalBERT}).
\newline
In addition to that, a third system which is built on a pre-trained language model for English Tweets \citep{RN683} has been implemented. We refer to this model as \textit{BERTweet}.

\subsection{PureBERT}
\label{sec:purebert}
The \textit{PureBERT} system leverages the BERT language model that was published by \citet{RN688}. BERT was pre-trained on the BooksCorpus \citep{RN689} and English Wikipedia and leverages the transformer architecture to provide contextualized representations for downstream tasks.
\newline
The \textit{PureBERT} model used in this research was implemented using Tensorflow \citep{RN681} and leverages the textual contents of the source tweets and as well as of their corresponding replies / retweets for rumour classification. 
\newline
\newline
\textbf{Pre-processing routine}
\newline
Prior to training the models on the given data, we have employed the following pre-processing procedure to clean the data and remove any Twitter-specific tokens:
\begin{enumerate}[noitemsep]
%\itemsep0em 
    \item For every Twitter event chain in the dataset, extract the source tweet text and corresponding replies and concatenate them 
    \item Remove URLs and user mentions from the tweet texts
    \item Convert tweet texts to lower-case
\end{enumerate}
For our experiments with \textit{PureBERT}, we have used a variety of pre-trained BERT models that have been made available on the HuggingFace Model Hub \footnote{HuggingFace Model Hub \url{https://huggingface.co/models}}. The following models were used for this task:
\begin{itemize}[noitemsep,topsep=2pt,parsep=0pt,partopsep=0pt]
    \item \verb|bert-base-uncased|
    \item \verb|bert-large-uncased|
    \item \verb|talkheads_ggelu_bert_en_base|\footnote{Pre-trained model from Tensorflow Hub \url{https://tfhub.dev/google/collections/transformer_encoders_text/1}}
\end{itemize}
The models \verb|bert-base-uncased| and \verb|bert-large-uncased| refer to the models that were published in the original BERT paper \citep{RN688}, while \verb|talkheads_ggelu_bert_en_base| refers to an improved BERT architecture proposed by \citet{RN685} and \citet{RN686}.
\newline
To provide input to the \textit{PureBERT} model, we first tokenize the input tweets via the \verb|BERTTokenizer|. For each tokenized input text.  we construct the following:
\begin{itemize}[noitemsep,topsep=2pt,parsep=0pt,partopsep=0pt]
    \item \textbf{input ids:} a sequence of integers identifying each input token to its index number in the \textit{PureBERT} tokenizer vocabulary
    \item \textbf{attention mask:} a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens
\end{itemize}
We subsequently fine-tune the pre-trained BERT models on the given input for $7$ epochs with AdamW optimizer \citep{RN687} using a learning rate of $3e^{-5}$, an epsilon value of $1e^{-8}$. We set the weight decay parameter to $1e^{-2}$.


\subsection{MultimodalBERT}
The \textit{MultimodalBERT} model is built using an experimental framework called Multimodal-Toolkit which has been made available on Github\footnote{Multimodal Transformers Repository \url{https://github.com/georgian-io/Multimodal-Toolkit}}. It allows to incorporate numerical and categorical features for downstream classification. 
\newline
The textual data used by the \textit{MultimodalBERT} system has been subject to the same pre-processing routine as described in section \ref{sec:purebert}. In addition to that, a variety of hand-crafted context metadata features have been produced. 
\newline
\newline
\textbf{Metadata Features}
\newline
The context metadata used by \textit{MultimodalBERT} can be categorized into tweet-level and user-level features. This approach was inspired by \cite{RN668} and is based on the observation that rumours may have different properties than non-rumours (e.g. rumours may be more likely to include links with unverified information). The full set of features that have been extracted is described in Table \ref{tbl:handcrafted_features}.
\begin{table}
\small
\setlength\tabcolsep{2pt}
\begin{tabularx}{0.48\textwidth}{XX}
\hline 
\textbf{Tweet-level features} &\textbf{User-level features}\\ \hline
Number of retweets & Number of total user posts\\
Number of favorites & Number of liked tweets \\
Occurrence of question mark & Number of followers\\
Occurrence of URLs in tweet & Number of followings \\
Number of embedded URLs & User verification status \\
Occurrence of media in tweet & Presence of geo-location \\
\hline
\end{tabularx}
\caption{Metadata features for \textit{MultimodalBERT}}
\label{tbl:handcrafted_features} 
\end{table}
In our experiments, we have used \textit{MultimodalBERT} with the \spverb|gating_on_cat_and_num_feats| configuration and trained the model for 7 epochs using the default configuration settings.


\subsection{BERTweet}
The \textit{BERTweet} model was published by \citet{RN683} and has been pre-trained on a corpus of 850M English Tweets. We put forward the hypothesis that the language that is used in tweets is fundamentally different from traditional text in terms of length and the use of informal language. Hence, a language model that is pre-trained on a large corpus of tweets should achieve a better performance compared to a generic language model pre-trained on more "formal" language like Wikipedia.
\newline
We have implemented the \textit{BERTweet} model using the PyTorch framework \citep{RN684}. Since BERTweet comes with its own tokenizer \verb|BertweetTokenizer| that supports raw Twitter data, no pre-processing has been applied to the tweet texts at all.
\newline
To provide input to the \textit{BERTweet} model, we first tokenize the input tweets via the tokenizer. For each tokenized input text, we again construct \textit{input ids} and \textit{attention masks} as described in section \ref{sec:purebert}.
\newline
We subsequently fine-tune the pre-trained BERTweet model on the given input for $7$ epochs with AdamW optimizer \citep{RN687} using a learning rate of $5e^{-5}$ and an epsilon value of $1e^{-8}$. We set the weight decay parameter to $1e^{-2}$.

\subsection{Results}
The results of the proposed models evaluated on the development set and the COMP90042 CodaLab competition are shown in Table \ref{tbl:evaluation_scores}.
\begin{table}
\centering
\small
\textbf{Development performance}
\setlength\tabcolsep{2pt}
\begin{tabular}{lccc}
\hline 
\textbf{Model / Features} & Precision & Recall & F1 Score \\ \hline
PureBert\textsubscript{base} & 76.47 & 79.73 & 75.21 \\ 
PureBert\textsubscript{large} & 77.14 & 86.17 & 81.41 \\ 
PureBert\textsubscript{talking heads} & 76.77 & 80.85 & 78.76 \\ %v39
BERTweet & 78.68 & 82.89 & 80.73 \\ %v21
MultimodalBERT & 72.44 & 87.63 & 79.32 \\ %v37 (local)
\hline \\
\end{tabular} 
\\
\textbf{Final evaluation performance}
\centering
\small
\setlength\tabcolsep{2pt}
\begin{tabular}{lccc}
\hline 
\textbf{Model / Features} & Precision & Recall & F1 Score \\ \hline
PureBert\textsubscript{base} & 83.52 & 80.85 & 82.16 \\ %v38
PureBert\textsubscript{large} & 81.25 & 82.98 & 82.11 \\ %v11
PureBert\textsubscript{talking heads} & 85.39 & 80.85 & 83.06 \\ %v16
\textbf{BERTweet} & 86.17 & 86.17 & \textbf{86.17} \\ %v36
MultimodalBERT  & 70.91 & 82.98 & 76.47 \\ %v37
\hline
\end{tabular}
\caption{Evaluation scores in \%}
\label{tbl:evaluation_scores}
\end{table}
From the evaluation scores, it is evident that increasing the number of layers  and number of parameters enhances the performance of BERT (\textit{PureBert\textsubscript{base}} vs. \textit{PureBert\textsubscript{large})}. We were able to further improve the performance of BERT with respect to the F1 score by leveraging an updated BERT architecture \textit{PureBert\textsubscript{talking heads}}.  
The \textit{MultimodalBERT} model performed worst in our experiments. This may indicate that the hand-crafted features used by this model were not sufficient indicators for rumours.
The model using the pre-trained \textit{BERTweet} model outperforms all other methods.
\newline
\begin{table*}[]
\small
\begin{tabularx}{\textwidth}{lXX}
 & \textbf{Rumour topics} & \textbf{Non-rumour topics} \\
 \hline
1 & \textit{covid people virus nigeria state amp time health follow let} & \textit{covid people amp like go good help think time thank} \\
2 & \textit{case new total recovery confirm report number bring today day} & \textit{trump american president coronavirus amp america lie response hoax donald} \\
3 & \textit{death number toll report cause figure coronavirus count covid week} & \textit{coronavirus people like spread think pandemic say know go die}  \\
4 & \textit{coronavirus spread china man corona lockdown think rule need good} & \textit{death die number covid cause rate flu people count toll}  \\
5 & \textit{test positive player day result member week testing covid negative} & \textit{china wuhan chinese world virus wuhanvirus ccp country lab pandemic} \\
\hline
\end{tabularx}
\caption{Topics of COVID-19 rumours and non-rumours (based on NMF)}
\label{tab:topics_of_covid-19}
\end{table*}
The above results support the hypothesis that the language that is used in tweets is fundamentally different from regular natural language and hence a custom model fine-tuned exclusively on tweets is more suited for the classification of Twitter posts than a language model trained on regular English Language.

\section{Task 2 - Rumour Analysis}
To  understand the nature COVID-19 rumours and how they differ to their non-rumour counterparts, we have first used our best-performing system \textit{BERTweet} to classify the COVID-19 dataset. We subsequently investigated the characteristics of COVID-19 rumours and non-rumours with regard to their topics, hashtags and associated sentiment.

\subsection{Topics discussed in rumour and non-rumour tweets}
To explore the topics that are being discussed in the tweets, a TF-IDF matrix was constructed on the given tweet texts. Subsequently, Non-Negative Matrix Factorization (NMF) was used to extract topics out of the TF-IDF matrix. Using NMF is beneficial because it decreases the impact of high-frequency words and hence helps us to obtain more specific topics.
\newline
Table \ref{tab:topics_of_covid-19} gives an overview of the top-5 topics discussed in rumour and non-rumour tweets. We can observe that rumour tweets are predominately referring to topics that are rather difficult to verify such as remote locations (e.g topic 1: \textit{virus nigeria}) or single persons (e.g. topic 5: \textit{test positive player}). Non-rumour events seem to refer to mostly fact-based topics that are easy to verify, such as topic 4: \textit{death number covid}.

\subsection{Hashtag usage in rumour and non-rumour tweets} 
With regard to the usage of hashtags, we observed that there is large overlap between rumour and non-rumours (e.g. hashtags like \#covid, \#coronovirus, \#trump are present in both classes). Nonetheless, we also identified several differences between the classes: Hashtags like \#hydroxychloroquine or \#breaking are frequently being used by rumour-spreading tweets, while hashtags like \#trump2020 or \#biden2020 are mostly present in tweets that are attributed to the non-rumour class.

\subsection{Expression of sentiment in rumour and non-rumour tweets}

In order to evaluate whether rumour and non-rumour tweets express different sentiments, we leveraged VADER (Valence Aware Dictionary and sEntiment Reasoner) from the NLTK library \citep{RN693}. VADER is a lexicon and rule-based sentiment analysis tool that can be used to predict a sentiment score ranging from -100\% for negative, 0\% for neutral and 100\% for positive sentiment. 
\begin{table}[]
\small
% \setlength\tabcolsep{2pt}
\begin{tabular}{lll}
\hline
\textbf{Rumour tweets} & Source tweets & Reply tweets \\ \hline
Compound & -9.37\% & -15.02\% \\
Positive & 6.50\% & 5.50\% \\
Neutral & 82.26\% & 87.05\% \\
Negative & 9.50\% & 7.45\% \\ \hline
\textbf{Non-rumour tweets} & Source tweets & Reply tweets \\ \hline
Compound & - 2.24\%  & -23.05\% \\
Positive & 8.71\% & 7.33\% \\
Neutral & 81.37\% & 83.16\% \\
Negative & 9.39\% & 9.49\%\\ \hline
\end{tabular}
\caption{Sentiment scores}
\label{tab:sentiment-scores}
\end{table}
We applied sentiment scoring on the source tweets and reply tweets for both rumour and non-rumour topics. The resulting (aggregated) scores are shown in Table \ref{tab:sentiment-scores}. 
\newline
The predicted compound sentiment for both the rumour and non-rumour events is negative, which indicates that COVID-19 is predominantly associated with negative sentiment. The source tweets of rumours are also significantly more negative than non-rumours with a compound sentiment score of -9.37\% vs. -2.24\%.
\newline
It is also interesting to observe that the reply tweets of both rumours and non-rumours are significantly more negative that the source tweets (-15.02\% and -23.05\% respectively). This indicates that a huge proportion of messages that are being shared in online communities may be of "toxic" nature.

\section{Conclusion}
In this paper, we have shown three rumour detection systems that make use of the BERT architecture. We studied their performance characteristics and used these systems to participate in the COMP90042 CodaLab competition where we achieved an F1-score of 86.17\% with our best-performing system \textit{BERTweet}. We used this systems to analyze tweets related to COVID-19 and to study the nature of rumours propagated on Twitter.


\newpage
\bibliographystyle{acl_natbib}
\bibliography{acl2021}

%\appendix



\end{document}
