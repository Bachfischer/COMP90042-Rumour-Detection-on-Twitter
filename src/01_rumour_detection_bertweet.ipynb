{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-europe",
   "metadata": {},
   "source": [
    "In order to run BERTweet, you will need to install latest version of transformers:\n",
    "* git clone https://github.com/huggingface/transformers.git\n",
    "* cd transformers\n",
    "* pip3 install --upgrade ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "subtle-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import (f1_score, recall_score, accuracy_score,\n",
    "                                precision_score)\n",
    "from transformers import (get_linear_schedule_with_warmup,AdamW,AutoModel, AutoTokenizer,\n",
    "                            AutoModelForSequenceClassification)\n",
    "from torch.utils.data import (TensorDataset,DataLoader,\n",
    "                             RandomSampler, SequentialSampler, Dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-sigma",
   "metadata": {},
   "source": [
    "Setup some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "important-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-seeker",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "For BERTweet we will only load the data and do not perform any preprocessing at all (even links + usernames will not be removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rural-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label):\n",
    "    if label == \"rumour\":\n",
    "        return 1\n",
    "    elif label == \"non-rumour\":\n",
    "        return 0\n",
    "    else:\n",
    "        raise Exception(\"label classes must be 'rumour' or 'non-rumour'\")\n",
    "        \n",
    "        \n",
    "def convert_prediction(pred):\n",
    "    if pred == 1:\n",
    "        return \"rumour\"\n",
    "    elif pred == 0:\n",
    "        return \"non-rumour\"\n",
    "    else:\n",
    "        raise Exception(\"prediction classes must be '0' or '1'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "athletic-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file, label_file):\n",
    "    \n",
    "    if label_file != None:\n",
    "        y_true = json.load(open(label_file))\n",
    "    \n",
    "    with open(data_file, 'r') as data_train:\n",
    "        raw_list = list(data_train)\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "\n",
    "    for event in raw_list:\n",
    "        tweets_in_event = json.loads(event)\n",
    "\n",
    "        tweet = {}\n",
    "\n",
    "        tweet['id'] = tweets_in_event[0]['id']\n",
    "        tweet['text'] = tweets_in_event[0]['text']\n",
    "        \n",
    "\n",
    "        # append text from follow-up tweets in tweet chain\n",
    "        follow_up_tweets = \"\"\n",
    "        for i in range(1, len(tweets_in_event)):\n",
    "            follow_up_tweets = follow_up_tweets + tweets_in_event[i]['text'] + \" \"\n",
    "        \n",
    "        # Concatenate text from all tweets in field 'text'\n",
    "        tweet['text'] = tweet['text'] + \" \" + follow_up_tweets\n",
    "\n",
    "        \n",
    "        tweet['text'] = tweet['text'].strip()\n",
    "        if label_file != None:\n",
    "            tweet['label'] = convert_label(y_true[str(tweet['id'])])\n",
    "        \n",
    "        data_list.append(tweet)\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prime-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data(data_file = '../data/train.data.jsonl', label_file = '../data/train.label.json')\n",
    "dev_df = load_data(data_file = '../data/dev.data.jsonl', label_file = '../data/dev.label.json')\n",
    "test_df = load_data(data_file = '../data/test.data.jsonl', label_file = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-topic",
   "metadata": {},
   "source": [
    "# BERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "orange-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(df, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sent in df[[\"text\"]].values:\n",
    "        sent = sent.item()\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      \n",
    "                            add_special_tokens = True, \n",
    "                            max_length = 128,           \n",
    "                            pad_to_max_length = True,\n",
    "                            truncation = True,\n",
    "                            return_attention_mask = True,   \n",
    "                            return_tensors = 'pt',    \n",
    "                    )\n",
    "           \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    inputs = {\n",
    "    'input_word_ids': input_ids,\n",
    "    'input_mask': attention_masks}\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "correct-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(train_df,test_df,dev_df,batch_size=8):\n",
    "    # Load the AutoTokenizer with a normalization mode if the input Tweet is raw\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
    "\n",
    "    tweet_valid = bert_encode(dev_df, tokenizer)\n",
    "    tweet_valid_labels = dev_df.label.astype(int)\n",
    "    \n",
    "    tweet_train = bert_encode(train_df, tokenizer)\n",
    "    tweet_train_labels = train_df.label.astype(int)\n",
    "    \n",
    "    tweet_test = bert_encode(test_df, tokenizer)\n",
    "\n",
    "\n",
    "    input_ids, attention_masks = tweet_train.values()\n",
    "    labels = torch.tensor(tweet_train_labels.values)\n",
    "    train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    \n",
    "    input_ids, attention_masks = tweet_valid.values()\n",
    "    labels = torch.tensor(tweet_valid_labels.values)\n",
    "    val_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    \n",
    "    input_ids, attention_masks = tweet_test.values()\n",
    "    test_dataset = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,\n",
    "                sampler = RandomSampler(train_dataset), \n",
    "                batch_size = batch_size \n",
    "            )\n",
    "\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "                val_dataset, \n",
    "                sampler = SequentialSampler(val_dataset),\n",
    "                batch_size = batch_size \n",
    "            )\n",
    "\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "                test_dataset, \n",
    "                sampler = SequentialSampler(test_dataset), \n",
    "                batch_size = batch_size\n",
    "            )\n",
    "    return train_dataloader,validation_dataloader,test_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "nasty-remains",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2096: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_dataloader,validation_dataloader,test_dataloader = prepare_dataloaders(train_df, test_df, dev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-beverage",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "spoken-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encode(sentence):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sentence,                      \n",
    "                        add_special_tokens = True,            \n",
    "                        pad_to_max_length = True,\n",
    "                        truncation = True,\n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'pt',    \n",
    "                )\n",
    "           \n",
    "    return encoded_dict['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "marine-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decode(tokens):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
    "    return tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "opponent-frank",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How to respond to the murderous attack on Charlie Hebdo? Every newspaper in the free world should print this. http://t.co/sC2ot63F6j @Heresy_Corner @KrustyAllslopp \\nJews label anyone they don't like as Anti-Semite and campaign until that person/company is finished. @Heresy_Corner @KrustyAllslopp \\nNo one does. @Heresy_Corner #ImCharlieHebdo @KrustyAllslopp Ditto @Grizzly_Stats @tom_wein What innocent Muslims ought to find insulting is an atrocity committed in their name, not a sodding cartoon. @Heresy_Corner @KrustyAllslopp \\nYes, until it becomes yours. @Heresy_Corner @KrustyAllslopp \\nWhy insult people who have nothing to do with this? People are genuinely offended by such drawings. @KrustyAllslopp @Heresy_Corner \\nAnd neither am I! I think this has little to do with actual Muslims. @berg_han Ah, you don't like Jews. Bye bye. @KrustyAllslopp @Heresy_Corner Also they kid you along with benign stuff then ... WHAM it's like a river of shite! @berg_han @Heresy_Corner It's a good point @Heresy_Corner @pjfny How about this? http://t.co/d2qcaVkf2h @Heresy_Corner @KrustyAllslopp \\nOrganised Jewry, I mean, not the actual people. Otherwise I'd be hating on my own ancestors. @theedwardian81 @Heresy_Corner ...and this: http://t.co/LmYxpmzw3v @Heresy_Corner @berg_han explored. @berg_han @KrustyAllslopp And if that's the case, that is your problem. @Heresy_Corner @tom_wein No point insulting billions of innocent muslims just to thumb our noses at a bunch of lunatics.They're not worth it @Heresy_Corner Oh dear... Just saw those tweets... Blocked him. @berg_han @KrustyAllslopp Because they have to learn to not be offended, that's why. @Heresy_Corner @berg_han But by that token Jews, Blacks and Irish people would have to 'learn' not to be offended either @Heresy_Corner @berg_han I get that ... I defend the right to free speech however there is a much broader context to this which is not @Heresy_Corner @berg_han just for the record. I am not in any way, shape or form defending this atrocity. @KrustyAllslopp @berg_han Yes, remind me when was the last time Jews bombed the Guardian. @Heresy_Corner I know!  Gives me the creeps. @Heresy_Corner There's a lot of very dodgy Twitter accounts who seem benign then you see the real side :( @Heresy_Corner @KrustyAllslopp \\nIf people insult something that's important to you, you feel that your identity is under attack. @KrustyAllslopp It's remarkable how quickly they come out the woodwork. @Heresy_Corner @EdzardErnst Why is the correct response to brutality to offend lots of people who *don't* support that brutality?\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fitted-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape      : torch.Size([1, 128])\n",
      "Word Ids   : tensor([[    0,   203,     9,  3924,     9,     6, 44766,  1545,    24,  5580,\n",
      "         44911,    32,    21,  1077,  8414,    16,     6,   336,   220,   151,\n",
      "          4197,    33,     4,    10,     5,     5,  7630,  6651,   400,    59,\n",
      "            32,    29,    43,    52,  7756,  9529,  2646,    13,  2184,   350,\n",
      "            25,   282,    75,  1151,    17,  1368,     4,     5,     5,   218,\n",
      "            63,   158,     4,     5,  8170,  5145, 24251, 44911,    32,     5,\n",
      "         47813,     5,     5,   165,  4520,  5636,  8789,     9,   259, 15386,\n",
      "            17,    74, 36802,   862,  6099,    16,   130,   330,     7,    46,\n",
      "            11,  2266, 32342,  9523,     4,     5,     5,   699,     7,   350,\n",
      "            18,  3655,  1103,     4,     5,     5,   250,  7967,    83,    87,\n",
      "            36,   349,     9,    32,    30,    33,    21,   603,    41,  5990,\n",
      "          7256,    61,   367, 17654,     4,     5,     5,   159,  4285,   155,\n",
      "             8,    12,     8,   101,    33,    90,   263,     2]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'How', 'to', 'respond', 'to', 'the', 'murderous', 'attack', 'on', 'Charlie', 'Heb@@', 'do', '?', 'Every', 'newspaper', 'in', 'the', 'free', 'world', 'should', 'print', 'this', '.', 'HTTPURL', '@USER', '@USER', 'Jews', 'label', 'anyone', 'they', 'do', \"n't\", 'like', 'as', 'Anti-@@', 'Sem@@', 'ite', 'and', 'campaign', 'until', 'that', 'person', '/', 'company', 'is', 'finished', '.', '@USER', '@USER', 'No', 'one', 'does', '.', '@USER', '#Im@@', 'Char@@', 'lie@@', 'Heb@@', 'do', '@USER', 'Ditto', '@USER', '@USER', 'What', 'innocent', 'Muslims', 'ought', 'to', 'find', 'insulting', 'is', 'an', 'atro@@', 'city', 'committed', 'in', 'their', 'name', ',', 'not', 'a', 'so@@', 'dding', 'cartoon', '.', '@USER', '@USER', 'Yes', ',', 'until', 'it', 'becomes', 'yours', '.', '@USER', '@USER', 'Why', 'insult', 'people', 'who', 'have', 'nothing', 'to', 'do', 'with', 'this', '?', 'People', 'are', 'genuinely', 'offended', 'by', 'such', 'drawings', '.', '@USER', '@USER', 'And', 'neither', 'am', 'I', '!', 'I', 'think', 'this', 'has', 'little', '</s>']\n"
     ]
    }
   ],
   "source": [
    "text_test = train_df.text[0]\n",
    "text_preprocessed = test_encode(text_test)\n",
    "\n",
    "\n",
    "print(f'Shape      : {text_preprocessed.shape}')\n",
    "print(f'Word Ids   : {text_preprocessed}')\n",
    "print(test_decode(text_preprocessed[0, :128]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-procurement",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "measured-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_class=\"vinai/bertweet-base\",num_classes=2,model_to_load=None,total_steps=-1):\n",
    "\n",
    "\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_class,\n",
    "        num_labels = num_classes,  \n",
    "        output_attentions = False, \n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                    lr = 5e-5,\n",
    "                    eps = 1e-8\n",
    "                    )\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, \n",
    "                                                num_training_steps = total_steps)\n",
    "\n",
    "    if model_to_load is not None:\n",
    "        try:\n",
    "            model.roberta.load_state_dict(torch.load(model_to_load))\n",
    "            print(\"LOADED MODEL\")\n",
    "        except:\n",
    "            pass\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "model, optimizer, scheduler = prepare_model(\"vinai/bertweet-base\" ,num_classes=2, model_to_load=None, total_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-activity",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,test_dataloader):\n",
    "    model.eval()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    t0 = time.time()\n",
    "    for batch in test_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        preds.append(logits)\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f} %\".format(avg_val_accuracy*100))\n",
    "    avg_val_loss = total_eval_loss / len(test_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    print(\"  Test Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Test took: {:}\".format(validation_time))\n",
    "    return preds, avg_val_accuracy, avg_val_loss, validation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,scheduler,train_dataloader,validation_dataloader,epochs):\n",
    "    seed_val = 42\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "        \n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()        \n",
    "            outputs = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask, \n",
    "                                labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "            \n",
    "        _, avg_val_accuracy, avg_val_loss, validation_time = validate(model,validation_dataloader)\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model,optimizer,scheduler,train_dataloader,validation_dataloader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu().roberta.state_dict(),\"./bertweet/bertweet_v21\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-consent",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test_dataloader):\n",
    "    model.eval()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        for logit in logits:\n",
    "            preds.append(logit)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(model,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "pred_labels = np.argmax(result, axis = 1)\n",
    "\n",
    "pred_scores = softmax(result, axis=1)[:, 1]\n",
    "\n",
    "predicted_labels = [convert_prediction(pred) for pred in pred_labels]\n",
    "\n",
    "output = pd.DataFrame({'id':test_df.id,'target':predicted_labels})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.Series(output.target.values,index=output.id).to_dict()\n",
    "with open('test-output.json', 'w') as f:\n",
    "    json.dump(submission, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-truth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "genuine-darwin",
   "metadata": {},
   "source": [
    "# BERTweet with merged dataset (i.e. train + dev data has been merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "engaging-luxury",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>552800070199148544</td>\n",
       "      <td>How to respond to the murderous attack on Char...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>544388259359387648</td>\n",
       "      <td>You can't condemn an entire race, nation or re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552805970536333314</td>\n",
       "      <td>Attempts to extend blame for this to all Musli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>525071376084791297</td>\n",
       "      <td>Rest in Peace, Cpl. Nathan Cirillo. Killed tod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>498355319979143168</td>\n",
       "      <td>People DEBATING whether #MikeBrown shoplifted ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>525025279803424768</td>\n",
       "      <td>The soldier shot dead in Wednesday's Ottawa at...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>552784600502915072</td>\n",
       "      <td>Charlie Hebdo became well known for publishing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5218</th>\n",
       "      <td>499696525808001024</td>\n",
       "      <td>We got through. That's a sniper on top of a ta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5219</th>\n",
       "      <td>580320612155060224</td>\n",
       "      <td>Last position of Germanwings flight #4U9525 at...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5220</th>\n",
       "      <td>553218279557582849</td>\n",
       "      <td>Kudos to Google for donating €250,000 to help ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5221 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                               text  \\\n",
       "0     552800070199148544  How to respond to the murderous attack on Char...   \n",
       "1     544388259359387648  You can't condemn an entire race, nation or re...   \n",
       "2     552805970536333314  Attempts to extend blame for this to all Musli...   \n",
       "3     525071376084791297  Rest in Peace, Cpl. Nathan Cirillo. Killed tod...   \n",
       "4     498355319979143168  People DEBATING whether #MikeBrown shoplifted ...   \n",
       "...                  ...                                                ...   \n",
       "5216  525025279803424768  The soldier shot dead in Wednesday's Ottawa at...   \n",
       "5217  552784600502915072  Charlie Hebdo became well known for publishing...   \n",
       "5218  499696525808001024  We got through. That's a sniper on top of a ta...   \n",
       "5219  580320612155060224  Last position of Germanwings flight #4U9525 at...   \n",
       "5220  553218279557582849  Kudos to Google for donating €250,000 to help ...   \n",
       "\n",
       "      label  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         1  \n",
       "4         0  \n",
       "...     ...  \n",
       "5216      1  \n",
       "5217      0  \n",
       "5218      0  \n",
       "5219      1  \n",
       "5220      0  \n",
       "\n",
       "[5221 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = train_df.append(dev_df, ignore_index = True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "neural-microwave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How to respond to the murderous attack on Charlie Hebdo? Every newspaper in the free world should print this. http://t.co/sC2ot63F6j @Heresy_Corner @KrustyAllslopp \\nJews label anyone they don't like as Anti-Semite and campaign until that person/company is finished. @Heresy_Corner @KrustyAllslopp \\nNo one does. @Heresy_Corner #ImCharlieHebdo @KrustyAllslopp Ditto @Grizzly_Stats @tom_wein What innocent Muslims ought to find insulting is an atrocity committed in their name, not a sodding cartoon. @Heresy_Corner @KrustyAllslopp \\nYes, until it becomes yours. @Heresy_Corner @KrustyAllslopp \\nWhy insult people who have nothing to do with this? People are genuinely offended by such drawings. @KrustyAllslopp @Heresy_Corner \\nAnd neither am I! I think this has little to do with actual Muslims. @berg_han Ah, you don't like Jews. Bye bye. @KrustyAllslopp @Heresy_Corner Also they kid you along with benign stuff then ... WHAM it's like a river of shite! @berg_han @Heresy_Corner It's a good point @Heresy_Corner @pjfny How about this? http://t.co/d2qcaVkf2h @Heresy_Corner @KrustyAllslopp \\nOrganised Jewry, I mean, not the actual people. Otherwise I'd be hating on my own ancestors. @theedwardian81 @Heresy_Corner ...and this: http://t.co/LmYxpmzw3v @Heresy_Corner @berg_han explored. @berg_han @KrustyAllslopp And if that's the case, that is your problem. @Heresy_Corner @tom_wein No point insulting billions of innocent muslims just to thumb our noses at a bunch of lunatics.They're not worth it @Heresy_Corner Oh dear... Just saw those tweets... Blocked him. @berg_han @KrustyAllslopp Because they have to learn to not be offended, that's why. @Heresy_Corner @berg_han But by that token Jews, Blacks and Irish people would have to 'learn' not to be offended either @Heresy_Corner @berg_han I get that ... I defend the right to free speech however there is a much broader context to this which is not @Heresy_Corner @berg_han just for the record. I am not in any way, shape or form defending this atrocity. @KrustyAllslopp @berg_han Yes, remind me when was the last time Jews bombed the Guardian. @Heresy_Corner I know!  Gives me the creeps. @Heresy_Corner There's a lot of very dodgy Twitter accounts who seem benign then you see the real side :( @Heresy_Corner @KrustyAllslopp \\nIf people insult something that's important to you, you feel that your identity is under attack. @KrustyAllslopp It's remarkable how quickly they come out the woodwork. @Heresy_Corner @EdzardErnst Why is the correct response to brutality to offend lots of people who *don't* support that brutality?\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "juvenile-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(combined_df,test_df,tokenizer_class=\"vinai/bertweet-base\",batch_size=8):\n",
    "    # Load the AutoTokenizer with a normalization mode if the input Tweet is raw\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_class, use_fast=False, normalization=True)\n",
    "\n",
    "    \n",
    "    tweet_train = bert_encode(combined_df, tokenizer)\n",
    "    tweet_train_labels = combined_df.label.astype(int)\n",
    "    \n",
    "    tweet_test = bert_encode(test_df, tokenizer)\n",
    "\n",
    "\n",
    "    input_ids, attention_masks = tweet_train.values()\n",
    "    labels = torch.tensor(tweet_train_labels.values)\n",
    "    train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    \n",
    "    input_ids, attention_masks = tweet_test.values()\n",
    "    test_dataset = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,\n",
    "                sampler = RandomSampler(train_dataset), \n",
    "                batch_size = batch_size \n",
    "            )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "                test_dataset, \n",
    "                sampler = SequentialSampler(test_dataset), \n",
    "                batch_size = batch_size\n",
    "            )\n",
    "    return train_dataloader,test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "latter-bones",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2096: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_dataloader,test_dataloader = prepare_dataloaders(combined_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "corresponding-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_class=\"vinai/bertweet-base\",num_classes=2,model_to_load=None,total_steps=-1):\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_class,\n",
    "        num_labels = num_classes,  \n",
    "        output_attentions = False, \n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                    lr = 5e-5,\n",
    "                    eps = 1e-8\n",
    "                    )\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, \n",
    "                                                num_training_steps = total_steps)\n",
    "\n",
    "    if model_to_load is not None:\n",
    "        try:\n",
    "            model.roberta.load_state_dict(torch.load(model_to_load))\n",
    "            print(\"LOADED MODEL\")\n",
    "        except:\n",
    "            pass\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "broke-transition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED MODEL\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "model, optimizer, scheduler = prepare_model(\"vinai/bertweet-base\" ,num_classes=2, model_to_load=\"./bertweet/bertweet_v18\", total_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cosmetic-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,scheduler,train_dataloader,epochs):\n",
    "    seed_val = 42\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "        \n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()        \n",
    "            outputs = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask, \n",
    "                                labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "assisted-spell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch    40  of    653.    Elapsed: 0:00:16.\n",
      "  Batch    80  of    653.    Elapsed: 0:00:33.\n",
      "  Batch   120  of    653.    Elapsed: 0:00:49.\n",
      "  Batch   160  of    653.    Elapsed: 0:01:05.\n",
      "  Batch   200  of    653.    Elapsed: 0:01:21.\n",
      "  Batch   240  of    653.    Elapsed: 0:01:38.\n",
      "  Batch   280  of    653.    Elapsed: 0:01:54.\n",
      "  Batch   320  of    653.    Elapsed: 0:02:10.\n",
      "  Batch   360  of    653.    Elapsed: 0:02:27.\n",
      "  Batch   400  of    653.    Elapsed: 0:02:43.\n",
      "  Batch   440  of    653.    Elapsed: 0:02:59.\n",
      "  Batch   480  of    653.    Elapsed: 0:03:16.\n",
      "  Batch   520  of    653.    Elapsed: 0:03:32.\n",
      "  Batch   560  of    653.    Elapsed: 0:03:48.\n",
      "  Batch   600  of    653.    Elapsed: 0:04:05.\n",
      "  Batch   640  of    653.    Elapsed: 0:04:21.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epcoh took: 0:04:26\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:04:26 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "train(model,optimizer,scheduler,train_dataloader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "wound-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu().roberta.state_dict(),\"./bertweet/bertweet_v35\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-saying",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "affiliated-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test_dataloader):\n",
    "    model.eval()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        for logit in logits:\n",
    "            preds.append(logit)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "finished-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(model,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "theoretical-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "pred_labels = np.argmax(result, axis = 1)\n",
    "\n",
    "pred_scores = softmax(result, axis=1)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "unlikely-television",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>544382249178001408</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>525027317551079424</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>544273220128739329</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>499571799764770816</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>552844104418091008</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>553581227165642752</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>552816302780579840</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>580350000074457088</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>498584409055174656</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>524961070465945600</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id      target\n",
       "0    544382249178001408      rumour\n",
       "1    525027317551079424      rumour\n",
       "2    544273220128739329      rumour\n",
       "3    499571799764770816  non-rumour\n",
       "4    552844104418091008  non-rumour\n",
       "..                  ...         ...\n",
       "576  553581227165642752      rumour\n",
       "577  552816302780579840  non-rumour\n",
       "578  580350000074457088      rumour\n",
       "579  498584409055174656  non-rumour\n",
       "580  524961070465945600  non-rumour\n",
       "\n",
       "[581 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = [convert_prediction(pred) for pred in pred_labels]\n",
    "\n",
    "output = pd.DataFrame({'id':test_df.id,'target':predicted_labels})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "continent-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.Series(output.target.values,index=output.id).to_dict()\n",
    "with open('test-output.json', 'w') as f:\n",
    "    json.dump(submission, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-bookmark",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-armenia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "drawn-installation",
   "metadata": {},
   "source": [
    "## Inference on COVID dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "multiple-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = load_data(data_file = '../data/covid.data.jsonl', label_file = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "solid-prevention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1272262651100434433</td>\n",
       "      <td>According to the New York Times, Warner Bros. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1287153210990395392</td>\n",
       "      <td>Hurricane Hanna has made landfall in Texas.\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1266555444283179008</td>\n",
       "      <td>Monkeys on the loose in India with stolen coro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1257715199655755779</td>\n",
       "      <td>Eastleigh and Swahili Arabs in Mombasa where c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1274338812173393920</td>\n",
       "      <td>“If Trump felt comfortable having it here, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17453</th>\n",
       "      <td>1249502859185590272</td>\n",
       "      <td>I wonder how many lives could’ve been saved if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17454</th>\n",
       "      <td>1284050414619459586</td>\n",
       "      <td>The @thetimes front page on 17th March. The fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17455</th>\n",
       "      <td>1274505289614725122</td>\n",
       "      <td>Trump just completed the racism trifecta in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17456</th>\n",
       "      <td>1267884642637676545</td>\n",
       "      <td>Here are a few of my photographs from today’s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17457</th>\n",
       "      <td>1265801718958301184</td>\n",
       "      <td>‘IT’S GONE’: Bill De Blasio Says NYC Facing $9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17458 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text\n",
       "0      1272262651100434433  According to the New York Times, Warner Bros. ...\n",
       "1      1287153210990395392  Hurricane Hanna has made landfall in Texas.\\n\\...\n",
       "2      1266555444283179008  Monkeys on the loose in India with stolen coro...\n",
       "3      1257715199655755779  Eastleigh and Swahili Arabs in Mombasa where c...\n",
       "4      1274338812173393920  “If Trump felt comfortable having it here, the...\n",
       "...                    ...                                                ...\n",
       "17453  1249502859185590272  I wonder how many lives could’ve been saved if...\n",
       "17454  1284050414619459586  The @thetimes front page on 17th March. The fi...\n",
       "17455  1274505289614725122  Trump just completed the racism trifecta in a ...\n",
       "17456  1267884642637676545  Here are a few of my photographs from today’s ...\n",
       "17457  1265801718958301184  ‘IT’S GONE’: Bill De Blasio Says NYC Facing $9...\n",
       "\n",
       "[17458 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "republican-trust",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2096: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_dataloader,covid_dataloader = prepare_dataloaders(combined_df, covid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "multiple-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(model,covid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "recreational-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "pred_labels = np.argmax(result, axis = 1)\n",
    "\n",
    "pred_scores = softmax(result, axis=1)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "competitive-serial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "solved-bouquet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1272262651100434433</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1287153210990395392</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1266555444283179008</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1257715199655755779</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1274338812173393920</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17453</th>\n",
       "      <td>1249502859185590272</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17454</th>\n",
       "      <td>1284050414619459586</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17455</th>\n",
       "      <td>1274505289614725122</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17456</th>\n",
       "      <td>1267884642637676545</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17457</th>\n",
       "      <td>1265801718958301184</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17458 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id      target\n",
       "0      1272262651100434433      rumour\n",
       "1      1287153210990395392  non-rumour\n",
       "2      1266555444283179008  non-rumour\n",
       "3      1257715199655755779  non-rumour\n",
       "4      1274338812173393920  non-rumour\n",
       "...                    ...         ...\n",
       "17453  1249502859185590272  non-rumour\n",
       "17454  1284050414619459586  non-rumour\n",
       "17455  1274505289614725122  non-rumour\n",
       "17456  1267884642637676545  non-rumour\n",
       "17457  1265801718958301184  non-rumour\n",
       "\n",
       "[17458 rows x 2 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = [convert_prediction(pred) for pred in pred_labels]\n",
    "\n",
    "output = pd.DataFrame({'id':covid_df.id,'target':predicted_labels})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "copyrighted-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.Series(output.target.values,index=output.id).to_dict()\n",
    "with open('covid-output.json', 'w') as f:\n",
    "    json.dump(submission, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-personality",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
