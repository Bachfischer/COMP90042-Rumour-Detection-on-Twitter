{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "arctic-presence",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extensive-conference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.7/site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-vacation",
   "metadata": {},
   "source": [
    "Note: In order to run BERTweet, you need to install the latest version of transformers:\n",
    "* `git clone https://github.com/huggingface/transformers.git`\n",
    "* `cd transformers`\n",
    "* `pip3 install --upgrade .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "considerable-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "from transformers import get_linear_schedule_with_warmup,AdamW,AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import TensorDataset,DataLoader, RandomSampler, SequentialSampler, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-myrtle",
   "metadata": {},
   "source": [
    "Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "black-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interested-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(preds, labels):\n",
    "    pred_flat = np.argmax(np.concatenate(preds), axis=1).flatten()\n",
    "    results = dict()\n",
    "    results['precision_score'] = precision_score(labels, pred_flat, average='binary')\n",
    "    results['recall_score'] = recall_score(labels, pred_flat, average='binary')\n",
    "    results['f1_score'] = f1_score(labels, pred_flat, average='binary')\n",
    "    return results\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-responsibility",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "For BERTweet we will only load the data and do not perform any preprocessing at all (even links + usernames will not be removed from the input that we feed to BERTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "above-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label):\n",
    "    if label == \"rumour\":\n",
    "        return 1\n",
    "    elif label == \"non-rumour\":\n",
    "        return 0\n",
    "    else:\n",
    "        raise Exception(\"label classes must be 'rumour' or 'non-rumour'\")\n",
    "        \n",
    "        \n",
    "def convert_prediction(pred):\n",
    "    if pred == 1:\n",
    "        return \"rumour\"\n",
    "    elif pred == 0:\n",
    "        return \"non-rumour\"\n",
    "    else:\n",
    "        raise Exception(\"prediction classes must be '0' or '1'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reduced-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file, label_file):\n",
    "    \n",
    "    if label_file != None:\n",
    "        y_true = json.load(open(label_file))\n",
    "    \n",
    "    with open(data_file, 'r') as data_train:\n",
    "        raw_list = list(data_train)\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "\n",
    "    for event in raw_list:\n",
    "        tweets_in_event = json.loads(event)\n",
    "\n",
    "        tweet = {}\n",
    "\n",
    "        tweet['id'] = tweets_in_event[0]['id']\n",
    "        tweet['text'] = tweets_in_event[0]['text']\n",
    "        \n",
    "        # append text from follow-up tweets in tweet chain\n",
    "        follow_up_tweets = \"\"\n",
    "        for i in range(1, len(tweets_in_event)):\n",
    "            follow_up_tweets = follow_up_tweets + tweets_in_event[i]['text'] + \" \"\n",
    "        \n",
    "        # Concatenate text from all tweets in the field 'text'\n",
    "        tweet['text'] = tweet['text'] + \" \" + follow_up_tweets\n",
    "        \n",
    "        tweet['text'] = tweet['text'].strip()\n",
    "        \n",
    "        if label_file != None:\n",
    "            tweet['label'] = convert_label(y_true[str(tweet['id'])])\n",
    "        \n",
    "        data_list.append(tweet)\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "blond-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data(data_file = '../data/train.data.jsonl', label_file = '../data/train.label.json')\n",
    "dev_df = load_data(data_file = '../data/dev.data.jsonl', label_file = '../data/dev.label.json')\n",
    "test_df = load_data(data_file = '../data/test.data.jsonl', label_file = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-customs",
   "metadata": {},
   "source": [
    "# BERTweet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "noticed-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(df, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sentence in df[[\"text\"]].values:\n",
    "        sentence = sentence.item()\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sentence,                      \n",
    "                            add_special_tokens = True,  \n",
    "                            max_length = 128,\n",
    "                            pad_to_max_length = True,\n",
    "                            truncation = True,\n",
    "                            return_attention_mask = True,   \n",
    "                            return_tensors = 'pt',    \n",
    "                    )\n",
    "           \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    inputs = {\n",
    "    'input_word_ids': input_ids,\n",
    "    'input_mask': attention_masks}\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hired-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(train_df,test_df,dev_df, batch_size):\n",
    "    # Load the AutoTokenizer with a normalization mode if the input Tweet is raw\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
    "\n",
    "    tweet_valid = bert_encode(dev_df, tokenizer)\n",
    "    tweet_valid_labels = dev_df.label.astype(int)\n",
    "    \n",
    "    tweet_train = bert_encode(train_df, tokenizer)\n",
    "    tweet_train_labels = train_df.label.astype(int)\n",
    "    \n",
    "    tweet_test = bert_encode(test_df, tokenizer)\n",
    "\n",
    "\n",
    "    input_ids, attention_masks = tweet_train.values()\n",
    "    labels = torch.tensor(tweet_train_labels.values)\n",
    "    train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    \n",
    "    input_ids, attention_masks = tweet_valid.values()\n",
    "    labels = torch.tensor(tweet_valid_labels.values)\n",
    "    val_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    \n",
    "    input_ids, attention_masks = tweet_test.values()\n",
    "    test_dataset = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,\n",
    "                sampler = RandomSampler(train_dataset), \n",
    "                batch_size = batch_size \n",
    "            )\n",
    "\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "                val_dataset, \n",
    "                sampler = SequentialSampler(val_dataset),\n",
    "                batch_size = batch_size \n",
    "            )\n",
    "\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "                test_dataset, \n",
    "                sampler = SequentialSampler(test_dataset), \n",
    "                batch_size = batch_size\n",
    "            )\n",
    "    \n",
    "    return train_dataloader,validation_dataloader,test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "crude-douglas",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2096: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_dataloader,validation_dataloader,test_dataloader = prepare_dataloaders(train_df, test_df, dev_df, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-windsor",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "severe-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encode(sentence):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sentence,                      \n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 128,\n",
    "                        pad_to_max_length = True,\n",
    "                        truncation = True,\n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'pt',    \n",
    "                )\n",
    "           \n",
    "    return encoded_dict['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eligible-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decode(tokens):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
    "    return tokenizer.convert_ids_to_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "taken-sacrifice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How to respond to the murderous attack on Charlie Hebdo? Every newspaper in the free world should print this. http://t.co/sC2ot63F6j @Heresy_Corner @KrustyAllslopp \\nJews label anyone they don't like as Anti-Semite and campaign until that person/company is finished. @Heresy_Corner @KrustyAllslopp \\nNo one does. @Heresy_Corner #ImCharlieHebdo @KrustyAllslopp Ditto @Grizzly_Stats @tom_wein What innocent Muslims ought to find insulting is an atrocity committed in their name, not a sodding cartoon. @Heresy_Corner @KrustyAllslopp \\nYes, until it becomes yours. @Heresy_Corner @KrustyAllslopp \\nWhy insult people who have nothing to do with this? People are genuinely offended by such drawings. @KrustyAllslopp @Heresy_Corner \\nAnd neither am I! I think this has little to do with actual Muslims. @berg_han Ah, you don't like Jews. Bye bye. @KrustyAllslopp @Heresy_Corner Also they kid you along with benign stuff then ... WHAM it's like a river of shite! @berg_han @Heresy_Corner It's a good point @Heresy_Corner @pjfny How about this? http://t.co/d2qcaVkf2h @Heresy_Corner @KrustyAllslopp \\nOrganised Jewry, I mean, not the actual people. Otherwise I'd be hating on my own ancestors. @theedwardian81 @Heresy_Corner ...and this: http://t.co/LmYxpmzw3v @Heresy_Corner @berg_han explored. @berg_han @KrustyAllslopp And if that's the case, that is your problem. @Heresy_Corner @tom_wein No point insulting billions of innocent muslims just to thumb our noses at a bunch of lunatics.They're not worth it @Heresy_Corner Oh dear... Just saw those tweets... Blocked him. @berg_han @KrustyAllslopp Because they have to learn to not be offended, that's why. @Heresy_Corner @berg_han But by that token Jews, Blacks and Irish people would have to 'learn' not to be offended either @Heresy_Corner @berg_han I get that ... I defend the right to free speech however there is a much broader context to this which is not @Heresy_Corner @berg_han just for the record. I am not in any way, shape or form defending this atrocity. @KrustyAllslopp @berg_han Yes, remind me when was the last time Jews bombed the Guardian. @Heresy_Corner I know!  Gives me the creeps. @Heresy_Corner There's a lot of very dodgy Twitter accounts who seem benign then you see the real side :( @Heresy_Corner @KrustyAllslopp \\nIf people insult something that's important to you, you feel that your identity is under attack. @KrustyAllslopp It's remarkable how quickly they come out the woodwork. @Heresy_Corner @EdzardErnst Why is the correct response to brutality to offend lots of people who *don't* support that brutality?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "opposite-things",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape      : torch.Size([1, 128])\n",
      "Word Ids   : tensor([[    0,   203,     9,  3924,     9,     6, 44766,  1545,    24,  5580,\n",
      "         44911,    32,    21,  1077,  8414,    16,     6,   336,   220,   151,\n",
      "          4197,    33,     4,    10,     5,     5,  7630,  6651,   400,    59,\n",
      "            32,    29,    43,    52,  7756,  9529,  2646,    13,  2184,   350,\n",
      "            25,   282,    75,  1151,    17,  1368,     4,     5,     5,   218,\n",
      "            63,   158,     4,     5,  8170,  5145, 24251, 44911,    32,     5,\n",
      "         47813,     5,     5,   165,  4520,  5636,  8789,     9,   259, 15386,\n",
      "            17,    74, 36802,   862,  6099,    16,   130,   330,     7,    46,\n",
      "            11,  2266, 32342,  9523,     4,     5,     5,   699,     7,   350,\n",
      "            18,  3655,  1103,     4,     5,     5,   250,  7967,    83,    87,\n",
      "            36,   349,     9,    32,    30,    33,    21,   603,    41,  5990,\n",
      "          7256,    61,   367, 17654,     4,     5,     5,   159,  4285,   155,\n",
      "             8,    12,     8,   101,    33,    90,   263,     2]])\n",
      "Decoded Words   : ['<s>', 'How', 'to', 'respond', 'to', 'the', 'murderous', 'attack', 'on', 'Charlie', 'Heb@@', 'do', '?', 'Every', 'newspaper', 'in', 'the', 'free', 'world', 'should', 'print', 'this', '.', 'HTTPURL', '@USER', '@USER', 'Jews', 'label', 'anyone', 'they', 'do', \"n't\", 'like', 'as', 'Anti-@@', 'Sem@@', 'ite', 'and', 'campaign', 'until', 'that', 'person', '/', 'company', 'is', 'finished', '.', '@USER', '@USER', 'No', 'one', 'does', '.', '@USER', '#Im@@', 'Char@@', 'lie@@', 'Heb@@', 'do', '@USER', 'Ditto', '@USER', '@USER', 'What', 'innocent', 'Muslims', 'ought', 'to', 'find', 'insulting', 'is', 'an', 'atro@@', 'city', 'committed', 'in', 'their', 'name', ',', 'not', 'a', 'so@@', 'dding', 'cartoon', '.', '@USER', '@USER', 'Yes', ',', 'until', 'it', 'becomes', 'yours', '.', '@USER', '@USER', 'Why', 'insult', 'people', 'who', 'have', 'nothing', 'to', 'do', 'with', 'this', '?', 'People', 'are', 'genuinely', 'offended', 'by', 'such', 'drawings', '.', '@USER', '@USER', 'And', 'neither', 'am', 'I', '!', 'I', 'think', 'this', 'has', 'little', '</s>']\n"
     ]
    }
   ],
   "source": [
    "text_test = train_df.text[0]\n",
    "text_encoded = test_encode(text_test)\n",
    "text_decoded = test_decode(text_encoded[0, :130])\n",
    "\n",
    "\n",
    "print(f'Shape      : {text_encoded.shape}')\n",
    "print(f'Word Ids   : {text_encoded}')\n",
    "print(f'Decoded Words   : {text_decoded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-destruction",
   "metadata": {},
   "source": [
    "## Prepare optimizer for BERTweet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hollow-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(num_classes, model_to_load=None, total_steps=-1):\n",
    "\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"vinai/bertweet-base\",\n",
    "        num_labels = num_classes,  \n",
    "        output_attentions = False, \n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                    lr = 5e-5,\n",
    "                    eps = 1e-8,\n",
    "                    weight_decay = 1e-2\n",
    "                    )\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, \n",
    "                                                num_training_steps = total_steps)\n",
    "\n",
    "    if model_to_load is not None:\n",
    "        model.roberta.load_state_dict(torch.load(model_to_load))\n",
    "        print(\"Loaded pre-trained model\")\n",
    "\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-aggregate",
   "metadata": {},
   "source": [
    "# BERTweet for development \n",
    "\n",
    "This BERTweet model was used for development purposes and makes use of the validation set to evaluate the performance on the given task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-cowboy",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aerial-active",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "epochs = 7\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "model, optimizer, scheduler = prepare_model(num_classes=2, model_to_load=None, total_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "studied-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,validation_dataloader, val_labels):\n",
    "    model.eval()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    preds = []\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        preds.append(logits)\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "    print(\"  Accuracy: {0:.3f} %\".format(avg_val_accuracy*100))\n",
    "    avg_val_loss = total_eval_loss / len(test_dataloader)\n",
    "    print(\"  Test Loss: {0:.3f}\".format(avg_val_loss))\n",
    "    \n",
    "    scores = calculate_scores(preds, val_labels)\n",
    "    print(\"  Precision Score: {0:.3f} %\".format(scores['precision_score']*100))\n",
    "    print(\"  Recall Score: {0:.3f} %\".format(scores['recall_score']*100))\n",
    "    print(\"  F1 Score: {0:.3f} %\".format(scores['f1_score']*100))\n",
    "\n",
    "    \n",
    "    return preds, avg_val_accuracy, avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "thermal-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, train_dataloader, validation_dataloader, val_labels, epochs):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training:')\n",
    "        \n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad() \n",
    "            \n",
    "            outputs = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask, \n",
    "                                labels=b_labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "            \n",
    "        _, avg_val_accuracy, avg_val_loss = validate(model,validation_dataloader, val_labels)\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "velvet-lighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 7 ========\n",
      "Training:\n",
      "  Batch    40  of  1,161.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  1,161.    Elapsed: 0:00:23.\n",
      "  Batch   120  of  1,161.    Elapsed: 0:00:34.\n",
      "  Batch   160  of  1,161.    Elapsed: 0:00:46.\n",
      "  Batch   200  of  1,161.    Elapsed: 0:00:57.\n",
      "  Batch   240  of  1,161.    Elapsed: 0:01:08.\n",
      "  Batch   280  of  1,161.    Elapsed: 0:01:19.\n",
      "  Batch   320  of  1,161.    Elapsed: 0:01:30.\n",
      "  Batch   360  of  1,161.    Elapsed: 0:01:41.\n",
      "  Batch   400  of  1,161.    Elapsed: 0:01:52.\n",
      "  Batch   440  of  1,161.    Elapsed: 0:02:03.\n",
      "  Batch   480  of  1,161.    Elapsed: 0:02:14.\n",
      "  Batch   520  of  1,161.    Elapsed: 0:02:25.\n",
      "  Batch   560  of  1,161.    Elapsed: 0:02:36.\n",
      "  Batch   600  of  1,161.    Elapsed: 0:02:47.\n",
      "  Batch   640  of  1,161.    Elapsed: 0:02:58.\n",
      "  Batch   680  of  1,161.    Elapsed: 0:03:09.\n",
      "  Batch   720  of  1,161.    Elapsed: 0:03:20.\n",
      "  Batch   760  of  1,161.    Elapsed: 0:03:32.\n",
      "  Batch   800  of  1,161.    Elapsed: 0:03:43.\n",
      "  Batch   840  of  1,161.    Elapsed: 0:03:54.\n",
      "  Batch   880  of  1,161.    Elapsed: 0:04:05.\n",
      "  Batch   920  of  1,161.    Elapsed: 0:04:16.\n",
      "  Batch   960  of  1,161.    Elapsed: 0:04:27.\n",
      "  Batch 1,000  of  1,161.    Elapsed: 0:04:38.\n",
      "  Batch 1,040  of  1,161.    Elapsed: 0:04:49.\n",
      "  Batch 1,080  of  1,161.    Elapsed: 0:05:00.\n",
      "  Batch 1,120  of  1,161.    Elapsed: 0:05:11.\n",
      "  Batch 1,160  of  1,161.    Elapsed: 0:05:22.\n",
      "\n",
      "  Training loss: 0.53\n",
      "  Training epoch took: 0:05:22\n",
      "  Accuracy: 85.103 %\n",
      "  Test Loss: 0.388\n",
      "  Precision Score: 80.952 %\n",
      "  Recall Score: 72.727 %\n",
      "  F1 Score: 76.620 %\n",
      "\n",
      "======== Epoch 2 / 7 ========\n",
      "Training:\n",
      "  Batch    40  of  1,161.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,161.    Elapsed: 0:00:22.\n",
      "  Batch   120  of  1,161.    Elapsed: 0:00:33.\n",
      "  Batch   160  of  1,161.    Elapsed: 0:00:44.\n",
      "  Batch   200  of  1,161.    Elapsed: 0:00:55.\n",
      "  Batch   240  of  1,161.    Elapsed: 0:01:06.\n",
      "  Batch   280  of  1,161.    Elapsed: 0:01:17.\n",
      "  Batch   320  of  1,161.    Elapsed: 0:01:28.\n",
      "  Batch   360  of  1,161.    Elapsed: 0:01:39.\n",
      "  Batch   400  of  1,161.    Elapsed: 0:01:50.\n",
      "  Batch   440  of  1,161.    Elapsed: 0:02:01.\n",
      "  Batch   480  of  1,161.    Elapsed: 0:02:12.\n",
      "  Batch   520  of  1,161.    Elapsed: 0:02:23.\n",
      "  Batch   560  of  1,161.    Elapsed: 0:02:34.\n",
      "  Batch   600  of  1,161.    Elapsed: 0:02:45.\n",
      "  Batch   640  of  1,161.    Elapsed: 0:02:56.\n",
      "  Batch   680  of  1,161.    Elapsed: 0:03:06.\n",
      "  Batch   720  of  1,161.    Elapsed: 0:03:17.\n",
      "  Batch   760  of  1,161.    Elapsed: 0:03:28.\n",
      "  Batch   800  of  1,161.    Elapsed: 0:03:39.\n",
      "  Batch   840  of  1,161.    Elapsed: 0:03:50.\n",
      "  Batch   880  of  1,161.    Elapsed: 0:04:01.\n",
      "  Batch   920  of  1,161.    Elapsed: 0:04:12.\n",
      "  Batch   960  of  1,161.    Elapsed: 0:04:23.\n",
      "  Batch 1,000  of  1,161.    Elapsed: 0:04:34.\n",
      "  Batch 1,040  of  1,161.    Elapsed: 0:04:45.\n",
      "  Batch 1,080  of  1,161.    Elapsed: 0:04:56.\n",
      "  Batch 1,120  of  1,161.    Elapsed: 0:05:07.\n",
      "  Batch 1,160  of  1,161.    Elapsed: 0:05:18.\n",
      "\n",
      "  Training loss: 0.42\n",
      "  Training epoch took: 0:05:18\n",
      "  Accuracy: 82.192 %\n",
      "  Test Loss: 0.497\n",
      "  Precision Score: 67.194 %\n",
      "  Recall Score: 90.909 %\n",
      "  F1 Score: 77.273 %\n",
      "\n",
      "======== Epoch 3 / 7 ========\n",
      "Training:\n",
      "  Batch    40  of  1,161.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,161.    Elapsed: 0:00:22.\n",
      "  Batch   120  of  1,161.    Elapsed: 0:00:33.\n",
      "  Batch   160  of  1,161.    Elapsed: 0:00:43.\n",
      "  Batch   200  of  1,161.    Elapsed: 0:00:54.\n",
      "  Batch   240  of  1,161.    Elapsed: 0:01:05.\n",
      "  Batch   280  of  1,161.    Elapsed: 0:01:16.\n",
      "  Batch   320  of  1,161.    Elapsed: 0:01:27.\n",
      "  Batch   360  of  1,161.    Elapsed: 0:01:38.\n",
      "  Batch   400  of  1,161.    Elapsed: 0:01:49.\n",
      "  Batch   440  of  1,161.    Elapsed: 0:02:00.\n",
      "  Batch   480  of  1,161.    Elapsed: 0:02:11.\n",
      "  Batch   520  of  1,161.    Elapsed: 0:02:22.\n",
      "  Batch   560  of  1,161.    Elapsed: 0:02:32.\n",
      "  Batch   600  of  1,161.    Elapsed: 0:02:43.\n",
      "  Batch   640  of  1,161.    Elapsed: 0:02:54.\n",
      "  Batch   680  of  1,161.    Elapsed: 0:03:05.\n",
      "  Batch   720  of  1,161.    Elapsed: 0:03:16.\n",
      "  Batch   760  of  1,161.    Elapsed: 0:03:27.\n",
      "  Batch   800  of  1,161.    Elapsed: 0:03:38.\n",
      "  Batch   840  of  1,161.    Elapsed: 0:03:48.\n",
      "  Batch   880  of  1,161.    Elapsed: 0:03:59.\n",
      "  Batch   920  of  1,161.    Elapsed: 0:04:10.\n",
      "  Batch   960  of  1,161.    Elapsed: 0:04:21.\n",
      "  Batch 1,000  of  1,161.    Elapsed: 0:04:32.\n",
      "  Batch 1,040  of  1,161.    Elapsed: 0:04:43.\n",
      "  Batch 1,080  of  1,161.    Elapsed: 0:04:54.\n",
      "  Batch 1,120  of  1,161.    Elapsed: 0:05:05.\n",
      "  Batch 1,160  of  1,161.    Elapsed: 0:05:16.\n",
      "\n",
      "  Training loss: 0.29\n",
      "  Training epoch took: 0:05:16\n",
      "  Accuracy: 88.014 %\n",
      "  Test Loss: 0.511\n",
      "  Precision Score: 80.711 %\n",
      "  Recall Score: 85.027 %\n",
      "  F1 Score: 82.812 %\n",
      "\n",
      "======== Epoch 4 / 7 ========\n",
      "Training:\n",
      "  Batch    40  of  1,161.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,161.    Elapsed: 0:00:22.\n",
      "  Batch   120  of  1,161.    Elapsed: 0:00:33.\n",
      "  Batch   160  of  1,161.    Elapsed: 0:00:43.\n",
      "  Batch   200  of  1,161.    Elapsed: 0:00:54.\n",
      "  Batch   240  of  1,161.    Elapsed: 0:01:05.\n",
      "  Batch   280  of  1,161.    Elapsed: 0:01:16.\n",
      "  Batch   320  of  1,161.    Elapsed: 0:01:27.\n",
      "  Batch   360  of  1,161.    Elapsed: 0:01:38.\n",
      "  Batch   400  of  1,161.    Elapsed: 0:01:49.\n",
      "  Batch   440  of  1,161.    Elapsed: 0:01:59.\n",
      "  Batch   480  of  1,161.    Elapsed: 0:02:10.\n",
      "  Batch   520  of  1,161.    Elapsed: 0:02:21.\n",
      "  Batch   560  of  1,161.    Elapsed: 0:02:32.\n",
      "  Batch   600  of  1,161.    Elapsed: 0:02:43.\n",
      "  Batch   640  of  1,161.    Elapsed: 0:02:54.\n",
      "  Batch   680  of  1,161.    Elapsed: 0:03:05.\n",
      "  Batch   720  of  1,161.    Elapsed: 0:03:15.\n",
      "  Batch   760  of  1,161.    Elapsed: 0:03:26.\n",
      "  Batch   800  of  1,161.    Elapsed: 0:03:37.\n",
      "  Batch   840  of  1,161.    Elapsed: 0:03:48.\n",
      "  Batch   880  of  1,161.    Elapsed: 0:03:59.\n",
      "  Batch   920  of  1,161.    Elapsed: 0:04:10.\n",
      "  Batch   960  of  1,161.    Elapsed: 0:04:21.\n",
      "  Batch 1,000  of  1,161.    Elapsed: 0:04:31.\n",
      "  Batch 1,040  of  1,161.    Elapsed: 0:04:42.\n",
      "  Batch 1,080  of  1,161.    Elapsed: 0:04:53.\n",
      "  Batch 1,120  of  1,161.    Elapsed: 0:05:04.\n",
      "  Batch 1,160  of  1,161.    Elapsed: 0:05:15.\n",
      "\n",
      "  Training loss: 0.22\n",
      "  Training epoch took: 0:05:15\n",
      "  Accuracy: 85.616 %\n",
      "  Test Loss: 0.546\n",
      "  Precision Score: 72.961 %\n",
      "  Recall Score: 90.909 %\n",
      "  F1 Score: 80.952 %\n",
      "\n",
      "======== Epoch 5 / 7 ========\n",
      "Training:\n",
      "  Batch    40  of  1,161.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,161.    Elapsed: 0:00:22.\n",
      "  Batch   120  of  1,161.    Elapsed: 0:00:32.\n",
      "  Batch   160  of  1,161.    Elapsed: 0:00:43.\n",
      "  Batch   200  of  1,161.    Elapsed: 0:00:54.\n",
      "  Batch   240  of  1,161.    Elapsed: 0:01:05.\n",
      "  Batch   280  of  1,161.    Elapsed: 0:01:16.\n",
      "  Batch   320  of  1,161.    Elapsed: 0:01:27.\n",
      "  Batch   360  of  1,161.    Elapsed: 0:01:37.\n",
      "  Batch   400  of  1,161.    Elapsed: 0:01:48.\n",
      "  Batch   440  of  1,161.    Elapsed: 0:01:59.\n",
      "  Batch   480  of  1,161.    Elapsed: 0:02:10.\n",
      "  Batch   520  of  1,161.    Elapsed: 0:02:21.\n",
      "  Batch   560  of  1,161.    Elapsed: 0:02:31.\n",
      "  Batch   600  of  1,161.    Elapsed: 0:02:42.\n",
      "  Batch   640  of  1,161.    Elapsed: 0:02:53.\n",
      "  Batch   680  of  1,161.    Elapsed: 0:03:04.\n",
      "  Batch   720  of  1,161.    Elapsed: 0:03:15.\n",
      "  Batch   760  of  1,161.    Elapsed: 0:03:26.\n",
      "  Batch   800  of  1,161.    Elapsed: 0:03:36.\n",
      "  Batch   840  of  1,161.    Elapsed: 0:03:47.\n",
      "  Batch   880  of  1,161.    Elapsed: 0:03:58.\n",
      "  Batch   920  of  1,161.    Elapsed: 0:04:09.\n",
      "  Batch   960  of  1,161.    Elapsed: 0:04:20.\n",
      "  Batch 1,000  of  1,161.    Elapsed: 0:04:31.\n",
      "  Batch 1,040  of  1,161.    Elapsed: 0:04:42.\n",
      "  Batch 1,080  of  1,161.    Elapsed: 0:04:52.\n",
      "  Batch 1,120  of  1,161.    Elapsed: 0:05:03.\n",
      "  Batch 1,160  of  1,161.    Elapsed: 0:05:14.\n",
      "\n",
      "  Training loss: 0.15\n",
      "  Training epoch took: 0:05:14\n",
      "  Accuracy: 89.212 %\n",
      "  Test Loss: 0.548\n",
      "  Precision Score: 86.782 %\n",
      "  Recall Score: 80.749 %\n",
      "  F1 Score: 83.657 %\n",
      "\n",
      "======== Epoch 6 / 7 ========\n",
      "Training:\n",
      "  Batch    40  of  1,161.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,161.    Elapsed: 0:00:22.\n",
      "  Batch   120  of  1,161.    Elapsed: 0:00:32.\n",
      "  Batch   160  of  1,161.    Elapsed: 0:00:43.\n",
      "  Batch   200  of  1,161.    Elapsed: 0:00:54.\n",
      "  Batch   240  of  1,161.    Elapsed: 0:01:05.\n",
      "  Batch   280  of  1,161.    Elapsed: 0:01:16.\n",
      "  Batch   320  of  1,161.    Elapsed: 0:01:26.\n",
      "  Batch   360  of  1,161.    Elapsed: 0:01:37.\n",
      "  Batch   400  of  1,161.    Elapsed: 0:01:48.\n",
      "  Batch   440  of  1,161.    Elapsed: 0:01:59.\n",
      "  Batch   480  of  1,161.    Elapsed: 0:02:10.\n",
      "  Batch   520  of  1,161.    Elapsed: 0:02:20.\n",
      "  Batch   560  of  1,161.    Elapsed: 0:02:31.\n",
      "  Batch   600  of  1,161.    Elapsed: 0:02:42.\n",
      "  Batch   640  of  1,161.    Elapsed: 0:02:53.\n",
      "  Batch   680  of  1,161.    Elapsed: 0:03:04.\n",
      "  Batch   720  of  1,161.    Elapsed: 0:03:14.\n",
      "  Batch   760  of  1,161.    Elapsed: 0:03:25.\n",
      "  Batch   800  of  1,161.    Elapsed: 0:03:36.\n",
      "  Batch   840  of  1,161.    Elapsed: 0:03:47.\n",
      "  Batch   880  of  1,161.    Elapsed: 0:03:57.\n",
      "  Batch   920  of  1,161.    Elapsed: 0:04:08.\n",
      "  Batch   960  of  1,161.    Elapsed: 0:04:19.\n",
      "  Batch 1,000  of  1,161.    Elapsed: 0:04:30.\n",
      "  Batch 1,040  of  1,161.    Elapsed: 0:04:41.\n",
      "  Batch 1,080  of  1,161.    Elapsed: 0:04:51.\n",
      "  Batch 1,120  of  1,161.    Elapsed: 0:05:02.\n",
      "  Batch 1,160  of  1,161.    Elapsed: 0:05:13.\n",
      "\n",
      "  Training loss: 0.11\n",
      "  Training epoch took: 0:05:13\n",
      "  Accuracy: 87.671 %\n",
      "  Test Loss: 0.662\n",
      "  Precision Score: 83.240 %\n",
      "  Recall Score: 79.679 %\n",
      "  F1 Score: 81.421 %\n",
      "\n",
      "======== Epoch 7 / 7 ========\n",
      "Training:\n",
      "  Batch    40  of  1,161.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,161.    Elapsed: 0:00:22.\n",
      "  Batch   120  of  1,161.    Elapsed: 0:00:32.\n",
      "  Batch   160  of  1,161.    Elapsed: 0:00:43.\n",
      "  Batch   200  of  1,161.    Elapsed: 0:00:54.\n",
      "  Batch   240  of  1,161.    Elapsed: 0:01:05.\n",
      "  Batch   280  of  1,161.    Elapsed: 0:01:15.\n",
      "  Batch   320  of  1,161.    Elapsed: 0:01:26.\n",
      "  Batch   360  of  1,161.    Elapsed: 0:01:37.\n",
      "  Batch   400  of  1,161.    Elapsed: 0:01:48.\n",
      "  Batch   440  of  1,161.    Elapsed: 0:01:58.\n",
      "  Batch   480  of  1,161.    Elapsed: 0:02:09.\n",
      "  Batch   520  of  1,161.    Elapsed: 0:02:20.\n",
      "  Batch   560  of  1,161.    Elapsed: 0:02:31.\n",
      "  Batch   600  of  1,161.    Elapsed: 0:02:41.\n",
      "  Batch   640  of  1,161.    Elapsed: 0:02:52.\n",
      "  Batch   680  of  1,161.    Elapsed: 0:03:03.\n",
      "  Batch   720  of  1,161.    Elapsed: 0:03:14.\n",
      "  Batch   760  of  1,161.    Elapsed: 0:03:25.\n",
      "  Batch   800  of  1,161.    Elapsed: 0:03:35.\n",
      "  Batch   840  of  1,161.    Elapsed: 0:03:46.\n",
      "  Batch   880  of  1,161.    Elapsed: 0:03:57.\n",
      "  Batch   920  of  1,161.    Elapsed: 0:04:08.\n",
      "  Batch   960  of  1,161.    Elapsed: 0:04:19.\n",
      "  Batch 1,000  of  1,161.    Elapsed: 0:04:30.\n",
      "  Batch 1,040  of  1,161.    Elapsed: 0:04:40.\n",
      "  Batch 1,080  of  1,161.    Elapsed: 0:04:51.\n",
      "  Batch 1,120  of  1,161.    Elapsed: 0:05:02.\n",
      "  Batch 1,160  of  1,161.    Elapsed: 0:05:13.\n",
      "\n",
      "  Training loss: 0.07\n",
      "  Training epoch took: 0:05:13\n",
      "  Accuracy: 86.644 %\n",
      "  Test Loss: 0.737\n",
      "  Precision Score: 78.680 %\n",
      "  Recall Score: 82.888 %\n",
      "  F1 Score: 80.729 %\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:37:57 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "train(model,optimizer,scheduler,train_dataloader,validation_dataloader, dev_df.label.astype(int), epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "parallel-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu().roberta.state_dict(),\"./bertweet/bertweet_v21\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-insurance",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "postal-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_dataloader):\n",
    "    model.eval()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        for logit in logits:\n",
    "            preds.append(logit)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "broken-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(model,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cellular-wonder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>544382249178001408</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>525027317551079424</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>544273220128739329</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>499571799764770816</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>552844104418091008</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>553581227165642752</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>552816302780579840</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>580350000074457088</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>498584409055174656</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>524961070465945600</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id      target\n",
       "0    544382249178001408      rumour\n",
       "1    525027317551079424      rumour\n",
       "2    544273220128739329      rumour\n",
       "3    499571799764770816  non-rumour\n",
       "4    552844104418091008  non-rumour\n",
       "..                  ...         ...\n",
       "576  553581227165642752  non-rumour\n",
       "577  552816302780579840  non-rumour\n",
       "578  580350000074457088      rumour\n",
       "579  498584409055174656  non-rumour\n",
       "580  524961070465945600      rumour\n",
       "\n",
       "[581 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "pred_labels = np.argmax(result, axis = 1)\n",
    "\n",
    "pred_scores = softmax(result, axis=1)[:, 1]\n",
    "\n",
    "predicted_labels = [convert_prediction(pred) for pred in pred_labels]\n",
    "\n",
    "output = pd.DataFrame({'id':test_df.id,'target':predicted_labels})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "unable-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.Series(output.target.values,index=output.id).to_dict()\n",
    "with open('test-output_v21.json', 'w') as f:\n",
    "    json.dump(submission, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-separate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-kentucky",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dying-electricity",
   "metadata": {},
   "source": [
    "# BERTweet for CodaLab submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-battle",
   "metadata": {},
   "source": [
    "For the CodaLab compeition, the train and development dataset has been merged to increase the size of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "imported-closing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>552800070199148544</td>\n",
       "      <td>How to respond to the murderous attack on Char...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>544388259359387648</td>\n",
       "      <td>You can't condemn an entire race, nation or re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552805970536333314</td>\n",
       "      <td>Attempts to extend blame for this to all Musli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>525071376084791297</td>\n",
       "      <td>Rest in Peace, Cpl. Nathan Cirillo. Killed tod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>498355319979143168</td>\n",
       "      <td>People DEBATING whether #MikeBrown shoplifted ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>525025279803424768</td>\n",
       "      <td>The soldier shot dead in Wednesday's Ottawa at...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>552784600502915072</td>\n",
       "      <td>Charlie Hebdo became well known for publishing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5218</th>\n",
       "      <td>499696525808001024</td>\n",
       "      <td>We got through. That's a sniper on top of a ta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5219</th>\n",
       "      <td>580320612155060224</td>\n",
       "      <td>Last position of Germanwings flight #4U9525 at...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5220</th>\n",
       "      <td>553218279557582849</td>\n",
       "      <td>Kudos to Google for donating â‚¬250,000 to help ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5221 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                               text  \\\n",
       "0     552800070199148544  How to respond to the murderous attack on Char...   \n",
       "1     544388259359387648  You can't condemn an entire race, nation or re...   \n",
       "2     552805970536333314  Attempts to extend blame for this to all Musli...   \n",
       "3     525071376084791297  Rest in Peace, Cpl. Nathan Cirillo. Killed tod...   \n",
       "4     498355319979143168  People DEBATING whether #MikeBrown shoplifted ...   \n",
       "...                  ...                                                ...   \n",
       "5216  525025279803424768  The soldier shot dead in Wednesday's Ottawa at...   \n",
       "5217  552784600502915072  Charlie Hebdo became well known for publishing...   \n",
       "5218  499696525808001024  We got through. That's a sniper on top of a ta...   \n",
       "5219  580320612155060224  Last position of Germanwings flight #4U9525 at...   \n",
       "5220  553218279557582849  Kudos to Google for donating â‚¬250,000 to help ...   \n",
       "\n",
       "      label  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         1  \n",
       "4         0  \n",
       "...     ...  \n",
       "5216      1  \n",
       "5217      0  \n",
       "5218      0  \n",
       "5219      1  \n",
       "5220      0  \n",
       "\n",
       "[5221 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = train_df.append(dev_df, ignore_index = True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "extended-nerve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How to respond to the murderous attack on Charlie Hebdo? Every newspaper in the free world should print this. http://t.co/sC2ot63F6j @Heresy_Corner @KrustyAllslopp \\nJews label anyone they don't like as Anti-Semite and campaign until that person/company is finished. @Heresy_Corner @KrustyAllslopp \\nNo one does. @Heresy_Corner #ImCharlieHebdo @KrustyAllslopp Ditto @Grizzly_Stats @tom_wein What innocent Muslims ought to find insulting is an atrocity committed in their name, not a sodding cartoon. @Heresy_Corner @KrustyAllslopp \\nYes, until it becomes yours. @Heresy_Corner @KrustyAllslopp \\nWhy insult people who have nothing to do with this? People are genuinely offended by such drawings. @KrustyAllslopp @Heresy_Corner \\nAnd neither am I! I think this has little to do with actual Muslims. @berg_han Ah, you don't like Jews. Bye bye. @KrustyAllslopp @Heresy_Corner Also they kid you along with benign stuff then ... WHAM it's like a river of shite! @berg_han @Heresy_Corner It's a good point @Heresy_Corner @pjfny How about this? http://t.co/d2qcaVkf2h @Heresy_Corner @KrustyAllslopp \\nOrganised Jewry, I mean, not the actual people. Otherwise I'd be hating on my own ancestors. @theedwardian81 @Heresy_Corner ...and this: http://t.co/LmYxpmzw3v @Heresy_Corner @berg_han explored. @berg_han @KrustyAllslopp And if that's the case, that is your problem. @Heresy_Corner @tom_wein No point insulting billions of innocent muslims just to thumb our noses at a bunch of lunatics.They're not worth it @Heresy_Corner Oh dear... Just saw those tweets... Blocked him. @berg_han @KrustyAllslopp Because they have to learn to not be offended, that's why. @Heresy_Corner @berg_han But by that token Jews, Blacks and Irish people would have to 'learn' not to be offended either @Heresy_Corner @berg_han I get that ... I defend the right to free speech however there is a much broader context to this which is not @Heresy_Corner @berg_han just for the record. I am not in any way, shape or form defending this atrocity. @KrustyAllslopp @berg_han Yes, remind me when was the last time Jews bombed the Guardian. @Heresy_Corner I know!  Gives me the creeps. @Heresy_Corner There's a lot of very dodgy Twitter accounts who seem benign then you see the real side :( @Heresy_Corner @KrustyAllslopp \\nIf people insult something that's important to you, you feel that your identity is under attack. @KrustyAllslopp It's remarkable how quickly they come out the woodwork. @Heresy_Corner @EdzardErnst Why is the correct response to brutality to offend lots of people who *don't* support that brutality?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unique-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(combined_df, test_df, batch_size):    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
    "\n",
    "    \n",
    "    tweet_train = bert_encode(combined_df, tokenizer)\n",
    "    tweet_train_labels = combined_df.label.astype(int)\n",
    "    \n",
    "    tweet_test = bert_encode(test_df, tokenizer)\n",
    "\n",
    "\n",
    "    input_ids, attention_masks = tweet_train.values()\n",
    "    labels = torch.tensor(tweet_train_labels.values)\n",
    "    train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    \n",
    "    input_ids, attention_masks = tweet_test.values()\n",
    "    test_dataset = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,\n",
    "                sampler = RandomSampler(train_dataset), \n",
    "                batch_size = batch_size \n",
    "            )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "                test_dataset, \n",
    "                sampler = SequentialSampler(test_dataset), \n",
    "                batch_size = batch_size\n",
    "            )\n",
    "    \n",
    "    return train_dataloader,test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "invalid-legislation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2096: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "combined_dataloader,test_dataloader = prepare_dataloaders(combined_df, test_df, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "certain-asset",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "epochs = 7\n",
    "total_steps = len(combined_dataloader) * epochs\n",
    "\n",
    "model, optimizer, scheduler = prepare_model(num_classes=2, model_to_load = None, total_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-brook",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "emotional-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,scheduler,train_dataloader,epochs):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training:')\n",
    "        \n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()        \n",
    "            outputs = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask, \n",
    "                                labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "instrumental-gauge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 7 ========\n",
      "Training:\n",
      "  Batch    40  of  1,161.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,161.    Elapsed: 0:00:22.\n",
      "  Batch   120  of  1,161.    Elapsed: 0:00:33.\n",
      "  Batch   160  of  1,161.    Elapsed: 0:00:44.\n",
      "  Batch   200  of  1,161.    Elapsed: 0:00:55.\n",
      "  Batch   240  of  1,161.    Elapsed: 0:01:06.\n",
      "  Batch   280  of  1,161.    Elapsed: 0:01:17.\n",
      "  Batch   320  of  1,161.    Elapsed: 0:01:29.\n",
      "  Batch   360  of  1,161.    Elapsed: 0:01:40.\n",
      "  Batch   400  of  1,161.    Elapsed: 0:01:51.\n",
      "  Batch   440  of  1,161.    Elapsed: 0:02:02.\n",
      "  Batch   480  of  1,161.    Elapsed: 0:02:13.\n",
      "  Batch   520  of  1,161.    Elapsed: 0:02:24.\n",
      "  Batch   560  of  1,161.    Elapsed: 0:02:35.\n",
      "  Batch   600  of  1,161.    Elapsed: 0:02:46.\n",
      "  Batch   640  of  1,161.    Elapsed: 0:02:57.\n",
      "  Batch   680  of  1,161.    Elapsed: 0:03:08.\n",
      "  Batch   720  of  1,161.    Elapsed: 0:03:19.\n",
      "  Batch   760  of  1,161.    Elapsed: 0:03:30.\n",
      "  Batch   800  of  1,161.    Elapsed: 0:03:41.\n",
      "  Batch   840  of  1,161.    Elapsed: 0:03:52.\n",
      "  Batch   880  of  1,161.    Elapsed: 0:04:03.\n",
      "  Batch   920  of  1,161.    Elapsed: 0:04:14.\n",
      "  Batch   960  of  1,161.    Elapsed: 0:04:25.\n",
      "  Batch 1,000  of  1,161.    Elapsed: 0:04:36.\n",
      "  Batch 1,040  of  1,161.    Elapsed: 0:04:47.\n",
      "  Batch 1,080  of  1,161.    Elapsed: 0:04:58.\n",
      "  Batch 1,120  of  1,161.    Elapsed: 0:05:09.\n",
      "  Batch 1,160  of  1,161.    Elapsed: 0:05:20.\n",
      "\n",
      "  Training loss: 0.55\n",
      "  Training epoch took: 0:05:20\n",
      "\n",
      "======== Epoch 2 / 7 ========\n",
      "Training:\n",
      "  Batch    40  of  1,161.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,161.    Elapsed: 0:00:22.\n",
      "  Batch   120  of  1,161.    Elapsed: 0:00:33.\n",
      "  Batch   160  of  1,161.    Elapsed: 0:00:44.\n",
      "  Batch   200  of  1,161.    Elapsed: 0:00:55.\n",
      "  Batch   240  of  1,161.    Elapsed: 0:01:06.\n",
      "  Batch   280  of  1,161.    Elapsed: 0:01:17.\n",
      "  Batch   320  of  1,161.    Elapsed: 0:01:27.\n",
      "  Batch   360  of  1,161.    Elapsed: 0:01:38.\n",
      "  Batch   400  of  1,161.    Elapsed: 0:01:49.\n",
      "  Batch   440  of  1,161.    Elapsed: 0:02:00.\n",
      "  Batch   480  of  1,161.    Elapsed: 0:02:11.\n",
      "  Batch   520  of  1,161.    Elapsed: 0:02:22.\n",
      "  Batch   560  of  1,161.    Elapsed: 0:02:33.\n",
      "  Batch   600  of  1,161.    Elapsed: 0:02:44.\n",
      "  Batch   640  of  1,161.    Elapsed: 0:02:55.\n",
      "  Batch   680  of  1,161.    Elapsed: 0:03:06.\n",
      "  Batch   720  of  1,161.    Elapsed: 0:03:17.\n",
      "  Batch   760  of  1,161.    Elapsed: 0:03:27.\n",
      "  Batch   800  of  1,161.    Elapsed: 0:03:38.\n",
      "  Batch   840  of  1,161.    Elapsed: 0:03:49.\n",
      "  Batch   880  of  1,161.    Elapsed: 0:04:00.\n",
      "  Batch   920  of  1,161.    Elapsed: 0:04:11.\n",
      "  Batch   960  of  1,161.    Elapsed: 0:04:22.\n",
      "  Batch 1,000  of  1,161.    Elapsed: 0:04:33.\n",
      "  Batch 1,040  of  1,161.    Elapsed: 0:04:44.\n",
      "  Batch 1,080  of  1,161.    Elapsed: 0:04:55.\n",
      "  Batch 1,120  of  1,161.    Elapsed: 0:05:06.\n",
      "  Batch 1,160  of  1,161.    Elapsed: 0:05:17.\n",
      "\n",
      "  Training loss: 0.41\n",
      "  Training epoch took: 0:05:17\n",
      "\n",
      "======== Epoch 3 / 7 ========\n",
      "Training:\n",
      "  Batch    40  of  1,161.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,161.    Elapsed: 0:00:22.\n",
      "  Batch   120  of  1,161.    Elapsed: 0:00:33.\n",
      "  Batch   160  of  1,161.    Elapsed: 0:00:43.\n",
      "  Batch   200  of  1,161.    Elapsed: 0:00:54.\n",
      "  Batch   240  of  1,161.    Elapsed: 0:01:05.\n",
      "  Batch   280  of  1,161.    Elapsed: 0:01:16.\n",
      "  Batch   320  of  1,161.    Elapsed: 0:01:27.\n",
      "  Batch   360  of  1,161.    Elapsed: 0:01:38.\n",
      "  Batch   400  of  1,161.    Elapsed: 0:01:49.\n",
      "  Batch   440  of  1,161.    Elapsed: 0:01:59.\n",
      "  Batch   480  of  1,161.    Elapsed: 0:02:10.\n",
      "  Batch   520  of  1,161.    Elapsed: 0:02:21.\n",
      "  Batch   560  of  1,161.    Elapsed: 0:02:32.\n",
      "  Batch   600  of  1,161.    Elapsed: 0:02:43.\n",
      "  Batch   640  of  1,161.    Elapsed: 0:02:54.\n",
      "  Batch   680  of  1,161.    Elapsed: 0:03:05.\n",
      "  Batch   720  of  1,161.    Elapsed: 0:03:16.\n",
      "  Batch   760  of  1,161.    Elapsed: 0:03:27.\n",
      "  Batch   800  of  1,161.    Elapsed: 0:03:37.\n",
      "  Batch   840  of  1,161.    Elapsed: 0:03:48.\n",
      "  Batch   880  of  1,161.    Elapsed: 0:03:59.\n",
      "  Batch   920  of  1,161.    Elapsed: 0:04:10.\n",
      "  Batch   960  of  1,161.    Elapsed: 0:04:21.\n",
      "  Batch 1,000  of  1,161.    Elapsed: 0:04:32.\n",
      "  Batch 1,040  of  1,161.    Elapsed: 0:04:43.\n",
      "  Batch 1,080  of  1,161.    Elapsed: 0:04:54.\n",
      "  Batch 1,120  of  1,161.    Elapsed: 0:05:05.\n",
      "  Batch 1,160  of  1,161.    Elapsed: 0:05:15.\n",
      "\n",
      "  Training loss: 0.36\n",
      "  Training epoch took: 0:05:16\n",
      "\n",
      "======== Epoch 4 / 7 ========\n",
      "Training:\n",
      "  Batch    40  of  1,161.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,161.    Elapsed: 0:00:22.\n",
      "  Batch   120  of  1,161.    Elapsed: 0:00:33.\n",
      "  Batch   160  of  1,161.    Elapsed: 0:00:43.\n",
      "  Batch   200  of  1,161.    Elapsed: 0:00:54.\n",
      "  Batch   240  of  1,161.    Elapsed: 0:01:05.\n",
      "  Batch   280  of  1,161.    Elapsed: 0:01:16.\n",
      "  Batch   320  of  1,161.    Elapsed: 0:01:27.\n",
      "  Batch   360  of  1,161.    Elapsed: 0:01:37.\n",
      "  Batch   400  of  1,161.    Elapsed: 0:01:48.\n",
      "  Batch   440  of  1,161.    Elapsed: 0:01:59.\n",
      "  Batch   480  of  1,161.    Elapsed: 0:02:10.\n",
      "  Batch   520  of  1,161.    Elapsed: 0:02:21.\n",
      "  Batch   560  of  1,161.    Elapsed: 0:02:32.\n",
      "  Batch   600  of  1,161.    Elapsed: 0:02:42.\n",
      "  Batch   640  of  1,161.    Elapsed: 0:02:53.\n",
      "  Batch   680  of  1,161.    Elapsed: 0:03:04.\n",
      "  Batch   720  of  1,161.    Elapsed: 0:03:15.\n",
      "  Batch   760  of  1,161.    Elapsed: 0:03:26.\n",
      "  Batch   800  of  1,161.    Elapsed: 0:03:37.\n",
      "  Batch   840  of  1,161.    Elapsed: 0:03:47.\n",
      "  Batch   880  of  1,161.    Elapsed: 0:03:58.\n",
      "  Batch   920  of  1,161.    Elapsed: 0:04:09.\n",
      "  Batch   960  of  1,161.    Elapsed: 0:04:20.\n",
      "  Batch 1,000  of  1,161.    Elapsed: 0:04:31.\n",
      "  Batch 1,040  of  1,161.    Elapsed: 0:04:41.\n",
      "  Batch 1,080  of  1,161.    Elapsed: 0:04:52.\n",
      "  Batch 1,120  of  1,161.    Elapsed: 0:05:03.\n",
      "  Batch 1,160  of  1,161.    Elapsed: 0:05:14.\n",
      "\n",
      "  Training loss: 0.25\n",
      "  Training epoch took: 0:05:14\n",
      "\n",
      "======== Epoch 5 / 7 ========\n",
      "Training:\n",
      "  Batch    40  of  1,161.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,161.    Elapsed: 0:00:22.\n",
      "  Batch   120  of  1,161.    Elapsed: 0:00:32.\n",
      "  Batch   160  of  1,161.    Elapsed: 0:00:43.\n",
      "  Batch   200  of  1,161.    Elapsed: 0:00:54.\n",
      "  Batch   240  of  1,161.    Elapsed: 0:01:05.\n",
      "  Batch   280  of  1,161.    Elapsed: 0:01:16.\n",
      "  Batch   320  of  1,161.    Elapsed: 0:01:26.\n",
      "  Batch   360  of  1,161.    Elapsed: 0:01:37.\n",
      "  Batch   400  of  1,161.    Elapsed: 0:01:48.\n",
      "  Batch   440  of  1,161.    Elapsed: 0:01:59.\n",
      "  Batch   480  of  1,161.    Elapsed: 0:02:10.\n",
      "  Batch   520  of  1,161.    Elapsed: 0:02:20.\n",
      "  Batch   560  of  1,161.    Elapsed: 0:02:31.\n",
      "  Batch   600  of  1,161.    Elapsed: 0:02:42.\n",
      "  Batch   640  of  1,161.    Elapsed: 0:02:53.\n",
      "  Batch   680  of  1,161.    Elapsed: 0:03:03.\n",
      "  Batch   720  of  1,161.    Elapsed: 0:03:14.\n",
      "  Batch   760  of  1,161.    Elapsed: 0:03:25.\n",
      "  Batch   800  of  1,161.    Elapsed: 0:03:36.\n",
      "  Batch   840  of  1,161.    Elapsed: 0:03:47.\n",
      "  Batch   880  of  1,161.    Elapsed: 0:03:57.\n",
      "  Batch   920  of  1,161.    Elapsed: 0:04:08.\n",
      "  Batch   960  of  1,161.    Elapsed: 0:04:19.\n",
      "  Batch 1,000  of  1,161.    Elapsed: 0:04:30.\n",
      "  Batch 1,040  of  1,161.    Elapsed: 0:04:40.\n",
      "  Batch 1,080  of  1,161.    Elapsed: 0:04:51.\n",
      "  Batch 1,120  of  1,161.    Elapsed: 0:05:02.\n",
      "  Batch 1,160  of  1,161.    Elapsed: 0:05:13.\n",
      "\n",
      "  Training loss: 0.16\n",
      "  Training epoch took: 0:05:13\n",
      "\n",
      "======== Epoch 6 / 7 ========\n",
      "Training:\n",
      "  Batch    40  of  1,161.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,161.    Elapsed: 0:00:21.\n",
      "  Batch   120  of  1,161.    Elapsed: 0:00:32.\n",
      "  Batch   160  of  1,161.    Elapsed: 0:00:43.\n",
      "  Batch   200  of  1,161.    Elapsed: 0:00:54.\n",
      "  Batch   240  of  1,161.    Elapsed: 0:01:05.\n",
      "  Batch   280  of  1,161.    Elapsed: 0:01:15.\n",
      "  Batch   320  of  1,161.    Elapsed: 0:01:26.\n",
      "  Batch   360  of  1,161.    Elapsed: 0:01:37.\n",
      "  Batch   400  of  1,161.    Elapsed: 0:01:48.\n",
      "  Batch   440  of  1,161.    Elapsed: 0:01:58.\n",
      "  Batch   480  of  1,161.    Elapsed: 0:02:09.\n",
      "  Batch   520  of  1,161.    Elapsed: 0:02:20.\n",
      "  Batch   560  of  1,161.    Elapsed: 0:02:31.\n",
      "  Batch   600  of  1,161.    Elapsed: 0:02:41.\n",
      "  Batch   640  of  1,161.    Elapsed: 0:02:52.\n",
      "  Batch   680  of  1,161.    Elapsed: 0:03:03.\n",
      "  Batch   720  of  1,161.    Elapsed: 0:03:14.\n",
      "  Batch   760  of  1,161.    Elapsed: 0:03:24.\n",
      "  Batch   800  of  1,161.    Elapsed: 0:03:35.\n",
      "  Batch   840  of  1,161.    Elapsed: 0:03:46.\n",
      "  Batch   880  of  1,161.    Elapsed: 0:03:57.\n",
      "  Batch   920  of  1,161.    Elapsed: 0:04:07.\n",
      "  Batch   960  of  1,161.    Elapsed: 0:04:18.\n",
      "  Batch 1,000  of  1,161.    Elapsed: 0:04:29.\n",
      "  Batch 1,040  of  1,161.    Elapsed: 0:04:40.\n",
      "  Batch 1,080  of  1,161.    Elapsed: 0:04:50.\n",
      "  Batch 1,120  of  1,161.    Elapsed: 0:05:01.\n",
      "  Batch 1,160  of  1,161.    Elapsed: 0:05:12.\n",
      "\n",
      "  Training loss: 0.10\n",
      "  Training epoch took: 0:05:12\n",
      "\n",
      "======== Epoch 7 / 7 ========\n",
      "Training:\n",
      "  Batch    40  of  1,161.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,161.    Elapsed: 0:00:21.\n",
      "  Batch   120  of  1,161.    Elapsed: 0:00:32.\n",
      "  Batch   160  of  1,161.    Elapsed: 0:00:43.\n",
      "  Batch   200  of  1,161.    Elapsed: 0:00:54.\n",
      "  Batch   240  of  1,161.    Elapsed: 0:01:04.\n",
      "  Batch   280  of  1,161.    Elapsed: 0:01:15.\n",
      "  Batch   320  of  1,161.    Elapsed: 0:01:26.\n",
      "  Batch   360  of  1,161.    Elapsed: 0:01:37.\n",
      "  Batch   400  of  1,161.    Elapsed: 0:01:47.\n",
      "  Batch   440  of  1,161.    Elapsed: 0:01:58.\n",
      "  Batch   480  of  1,161.    Elapsed: 0:02:09.\n",
      "  Batch   520  of  1,161.    Elapsed: 0:02:20.\n",
      "  Batch   560  of  1,161.    Elapsed: 0:02:30.\n",
      "  Batch   600  of  1,161.    Elapsed: 0:02:41.\n",
      "  Batch   640  of  1,161.    Elapsed: 0:02:52.\n",
      "  Batch   680  of  1,161.    Elapsed: 0:03:03.\n",
      "  Batch   720  of  1,161.    Elapsed: 0:03:13.\n",
      "  Batch   760  of  1,161.    Elapsed: 0:03:24.\n",
      "  Batch   800  of  1,161.    Elapsed: 0:03:35.\n",
      "  Batch   840  of  1,161.    Elapsed: 0:03:46.\n",
      "  Batch   880  of  1,161.    Elapsed: 0:03:56.\n",
      "  Batch   920  of  1,161.    Elapsed: 0:04:07.\n",
      "  Batch   960  of  1,161.    Elapsed: 0:04:18.\n",
      "  Batch 1,000  of  1,161.    Elapsed: 0:04:29.\n",
      "  Batch 1,040  of  1,161.    Elapsed: 0:04:39.\n",
      "  Batch 1,080  of  1,161.    Elapsed: 0:04:50.\n",
      "  Batch 1,120  of  1,161.    Elapsed: 0:05:01.\n",
      "  Batch 1,160  of  1,161.    Elapsed: 0:05:12.\n",
      "\n",
      "  Training loss: 0.07\n",
      "  Training epoch took: 0:05:12\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:36:43 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "train(model,optimizer,scheduler,train_dataloader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "rolled-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu().roberta.state_dict(),\"./bertweet/bertweet_v35\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-configuration",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "surrounded-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test_dataloader):\n",
    "    model.eval()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        for logit in logits:\n",
    "            preds.append(logit)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "solved-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(model,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "naked-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "pred_labels = np.argmax(result, axis = 1)\n",
    "\n",
    "pred_scores = softmax(result, axis=1)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "after-pearl",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>544382249178001408</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>525027317551079424</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>544273220128739329</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>499571799764770816</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>552844104418091008</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>553581227165642752</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>552816302780579840</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>580350000074457088</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>498584409055174656</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>524961070465945600</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id      target\n",
       "0    544382249178001408      rumour\n",
       "1    525027317551079424      rumour\n",
       "2    544273220128739329      rumour\n",
       "3    499571799764770816  non-rumour\n",
       "4    552844104418091008  non-rumour\n",
       "..                  ...         ...\n",
       "576  553581227165642752  non-rumour\n",
       "577  552816302780579840  non-rumour\n",
       "578  580350000074457088      rumour\n",
       "579  498584409055174656  non-rumour\n",
       "580  524961070465945600  non-rumour\n",
       "\n",
       "[581 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = [convert_prediction(pred) for pred in pred_labels]\n",
    "\n",
    "output = pd.DataFrame({'id':test_df.id,'target':predicted_labels})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "opponent-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.Series(output.target.values,index=output.id).to_dict()\n",
    "with open('test-output.json', 'w') as f:\n",
    "    json.dump(submission, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-cursor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "christian-sitting",
   "metadata": {},
   "source": [
    "# Perform Inference on COVID dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "contrary-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = load_data(data_file = '../data/covid.data.jsonl', label_file = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aggressive-reporter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1272262651100434433</td>\n",
       "      <td>According to the New York Times, Warner Bros. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1287153210990395392</td>\n",
       "      <td>Hurricane Hanna has made landfall in Texas.\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1266555444283179008</td>\n",
       "      <td>Monkeys on the loose in India with stolen coro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1257715199655755779</td>\n",
       "      <td>Eastleigh and Swahili Arabs in Mombasa where c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1274338812173393920</td>\n",
       "      <td>â€œIf Trump felt comfortable having it here, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17453</th>\n",
       "      <td>1249502859185590272</td>\n",
       "      <td>I wonder how many lives couldâ€™ve been saved if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17454</th>\n",
       "      <td>1284050414619459586</td>\n",
       "      <td>The @thetimes front page on 17th March. The fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17455</th>\n",
       "      <td>1274505289614725122</td>\n",
       "      <td>TrumpÂ just completed the racism trifecta in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17456</th>\n",
       "      <td>1267884642637676545</td>\n",
       "      <td>Here are a few of my photographs from todayâ€™s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17457</th>\n",
       "      <td>1265801718958301184</td>\n",
       "      <td>â€˜ITâ€™S GONEâ€™: Bill De Blasio Says NYC Facing $9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17458 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text\n",
       "0      1272262651100434433  According to the New York Times, Warner Bros. ...\n",
       "1      1287153210990395392  Hurricane Hanna has made landfall in Texas.\\n\\...\n",
       "2      1266555444283179008  Monkeys on the loose in India with stolen coro...\n",
       "3      1257715199655755779  Eastleigh and Swahili Arabs in Mombasa where c...\n",
       "4      1274338812173393920  â€œIf Trump felt comfortable having it here, the...\n",
       "...                    ...                                                ...\n",
       "17453  1249502859185590272  I wonder how many lives couldâ€™ve been saved if...\n",
       "17454  1284050414619459586  The @thetimes front page on 17th March. The fi...\n",
       "17455  1274505289614725122  TrumpÂ just completed the racism trifecta in a ...\n",
       "17456  1267884642637676545  Here are a few of my photographs from todayâ€™s ...\n",
       "17457  1265801718958301184  â€˜ITâ€™S GONEâ€™: Bill De Blasio Says NYC Facing $9...\n",
       "\n",
       "[17458 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "tamil-playback",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2096: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_dataloader,covid_dataloader = prepare_dataloaders(combined_df, covid_df, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "educational-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(model,covid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "studied-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "pred_labels = np.argmax(result, axis = 1)\n",
    "\n",
    "pred_scores = softmax(result, axis=1)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "careful-innocent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "corresponding-poker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1272262651100434433</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1287153210990395392</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1266555444283179008</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1257715199655755779</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1274338812173393920</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17453</th>\n",
       "      <td>1249502859185590272</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17454</th>\n",
       "      <td>1284050414619459586</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17455</th>\n",
       "      <td>1274505289614725122</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17456</th>\n",
       "      <td>1267884642637676545</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17457</th>\n",
       "      <td>1265801718958301184</td>\n",
       "      <td>non-rumour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17458 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id      target\n",
       "0      1272262651100434433      rumour\n",
       "1      1287153210990395392  non-rumour\n",
       "2      1266555444283179008  non-rumour\n",
       "3      1257715199655755779  non-rumour\n",
       "4      1274338812173393920  non-rumour\n",
       "...                    ...         ...\n",
       "17453  1249502859185590272  non-rumour\n",
       "17454  1284050414619459586  non-rumour\n",
       "17455  1274505289614725122  non-rumour\n",
       "17456  1267884642637676545  non-rumour\n",
       "17457  1265801718958301184  non-rumour\n",
       "\n",
       "[17458 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = [convert_prediction(pred) for pred in pred_labels]\n",
    "\n",
    "output = pd.DataFrame({'id':covid_df.id,'target':predicted_labels})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ranking-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.Series(output.target.values,index=output.id).to_dict()\n",
    "with open('covid-output.json', 'w') as f:\n",
    "    json.dump(submission, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-worry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
