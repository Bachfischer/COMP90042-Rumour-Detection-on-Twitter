{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-venice",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data(data_file = '../data/train.data.jsonl', label_file = '../data/train.label.json', perform_stemming = False)\n",
    "dev_df = load_data(data_file = '../data/dev.data.jsonl', label_file = '../data/dev.label.json', perform_stemming = False)\n",
    "test_df = load_data(data_file = '../data/test.data.jsonl', label_file = None, perform_stemming = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-parking",
   "metadata": {},
   "source": [
    "## Text-only BERT with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optmizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seed = 42\n",
    "class_names = [\"non-rumour\", \"rumour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df.text, train_df.label.values))\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((dev_df.text, dev_df.label.values))\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "\n",
    "#combined_ds = tf.data.Dataset.from_tensor_slices((combined_df.text, combined_df.label.values))\n",
    "#combined_ds = combined_ds.batch(batch_size)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_df.text))\n",
    "test_ds = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = 'talkheads_ggelu_bert_en_base' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'talkheads_ggelu_bert_en_large':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1',\n",
    "    'bert_en_uncased_L-24_H-1024_A-16':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4',\n",
    "    'talkheads_ggelu_bert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talkheads_ggelu_bert_en_large':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_uncased_L-24_H-1024_A-16':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talkheads_ggelu_bert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-horror",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "split-continent",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = [train_df['text'][0]]\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-advocacy",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    \n",
    "    preprocessor = hub.load(tfhub_handle_preprocess)\n",
    "    \n",
    "    text_inputs = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    \n",
    "    tokenize = hub.KerasLayer(preprocessor.tokenize)\n",
    "    \n",
    "    tokenized_inputs = tokenize(text_inputs)\n",
    "    \n",
    "    # pack input sequences for the Transformer encoder.\n",
    "    seq_length = 512  # Your choice here.\n",
    "    bert_pack_inputs = hub.KerasLayer(\n",
    "        preprocessing_layer.bert_pack_inputs,\n",
    "        arguments=dict(seq_length=seq_length))  # Optional argument.\n",
    "    \n",
    "    encoder_inputs = bert_pack_inputs(tokenized_inputs)\n",
    "\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    \n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = [tf.metrics.BinaryAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "#steps_per_epoch = tf.data.experimental.cardinality(combined_ds).numpy()\n",
    "\n",
    "\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)\n",
    "\n",
    "#history = classifier_model.fit(x=combined_ds,\n",
    "#                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-glossary",
   "metadata": {},
   "source": [
    "### Plot accuracy and loss over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.save(\"./pure_bert/pure_bert_v32\", include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-summit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "imperial-skating",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tf.sigmoid(classifier_model.predict(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.round(result).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = [convert_prediction(pred) for pred in result]\n",
    "output = pd.DataFrame({'id':test_df.id,'target':predicted_labels})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.Series(output.target.values,index=output.id).to_dict()\n",
    "with open('test-output.json', 'w') as f:\n",
    "    json.dump(submission, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-lending",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-classroom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-envelope",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-cathedral",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
